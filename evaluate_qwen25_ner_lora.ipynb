{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate QLoRA NER on Test Set (BIO)\n",
        "\n",
        "This notebook loads the base `Qwen2.5-0.5B` with the trained LoRA adapter, generates BIO tag sequences for test instructions, and computes per-entity TP/FP/FN and precision/recall/F1, plus a seqeval classification report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Torch: 2.5.1+cu121 CUDA: 12.1 is_available: True\n"
          ]
        }
      ],
      "source": [
        "%pip install -q evaluate==0.4.3 seqeval==1.2.2\n",
        "import torch, os, json, re\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "print('Torch:', torch.__version__, 'CUDA:', torch.version.cuda, 'is_available:', torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded base + LoRA from: models/Qwen2.5-0.5B outputs\\qwen25-0.5b-qlora-ner\\lora_adapter\n"
          ]
        }
      ],
      "source": [
        "model_dir = \"models/Qwen2.5-0.5B\"\n",
        "adapter_dir = os.path.join(\"outputs\", \"qwen25-0.5b-qlora-ner\", \"lora_adapter\")\n",
        "\n",
        "tok_path = os.path.join(adapter_dir, \"tokenizer\")\n",
        "if os.path.isdir(tok_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tok_path, use_fast=True, trust_remote_code=True)\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "device_map = \"auto\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "try:\n",
        "    base_model.resize_token_embeddings(len(tokenizer))\n",
        "except Exception as e:\n",
        "    print(\"resize_token_embeddings skipped:\", e)\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, adapter_dir, is_trainable=False, ignore_mismatched_sizes=True)\n",
        "model.eval()\n",
        "print(\"Loaded base + LoRA from:\", model_dir, adapter_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['id', 'source', 'text', 'tokens', 'labels', 'instruction', 'response'])\n",
            "Loaded 10 test examples\n"
          ]
        }
      ],
      "source": [
        "test_path = \"outputs/standardized_gretel_pii_masking_en_test.jsonl\"\n",
        "\n",
        "ds = load_dataset(\"json\", data_files=test_path, split=\"train\")\n",
        "\n",
        "print(ds[0].keys())\n",
        "\n",
        "texts = [ex[\"instruction\"] for ex in ds]\n",
        "# Parse per-line BIO tags (one label per line)\n",
        "labels_true = [re.findall(r\"^(?:B|I)-[A-Za-z0-9_]+|^O$\", ex[\"response\"], flags=re.M) for ex in ds]\n",
        "\n",
        "print(\"Loaded\", len(texts), \"test examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "141b3260ad674236bbdace2de4717342",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example prediction:\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "gen_kwargs = dict(\n",
        "    max_new_tokens=512,  # allow enough lines\n",
        "    do_sample=False,\n",
        "    num_beams=1,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "pred_sequences = []\n",
        "\n",
        "for text in tqdm(texts):\n",
        "    prompt = text\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad(), torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "        outputs = model.generate(**inputs, **gen_kwargs)\n",
        "    gen_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "    # Parse one tag per line after \"Labels:\"; still robust to stray prose\n",
        "    tags = re.findall(r\"^(?:B|I)-[A-Za-z0-9_]+|^O$\", gen_text, flags=re.M)\n",
        "    pred_sequences.append(tags)\n",
        "\n",
        "print(\"Example prediction:\")\n",
        "print(pred_sequences[0][:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aligned sequences prepared.\n",
            "Example true len, pred len: 81 81\n"
          ]
        }
      ],
      "source": [
        "# Align lengths by truncating/padding with 'O'\n",
        "true_aligned, pred_aligned = [], []\n",
        "for ex, (instr, t, p) in enumerate(zip(texts, labels_true, pred_sequences)):\n",
        "    # Infer the intended token count from the instruction (count lines after 'Tokens:')\n",
        "    tokens_match = re.split(r\"\\bTokens:\\s*\\n\", instr)\n",
        "    expected = None\n",
        "    if len(tokens_match) > 1:\n",
        "        # Up to 'Labels:' marker\n",
        "        token_block = tokens_match[1].split(\"\\n\\nLabels:\", 1)[0]\n",
        "        expected = len([ln for ln in token_block.splitlines() if ln.strip() != \"\"])\n",
        "    L = expected or max(len(t), len(p))\n",
        "    t_adj = (t + [\"O\"] * (L - len(t)))[:L]\n",
        "    p_adj = (p + [\"O\"] * (L - len(p)))[:L]\n",
        "    true_aligned.append(t_adj)\n",
        "    pred_aligned.append(p_adj)\n",
        "\n",
        "print(\"Aligned sequences prepared.\")\n",
        "print(\"Example true len, pred len:\", len(true_aligned[0]), len(pred_aligned[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Saved: outputs\\ner_eval_per_type_exact.csv\n"
          ]
        }
      ],
      "source": [
        "%pip install -q nervaluate pandas\n",
        "import pandas as pd\n",
        "from nervaluate import Evaluator\n",
        "\n",
        "valid = re.compile(r'^(?:(?:B|I)-[A-Za-z0-9_]+|O)$')\n",
        "true_clean = [[t if valid.match(t) else 'O' for t in seq] for seq in true_aligned]\n",
        "pred_clean = [[t if valid.match(t) else 'O' for t in seq] for seq in pred_aligned]\n",
        "\n",
        "entity_types = sorted({t.split('-', 1)[1] for seq in (true_clean + pred_clean) for t in seq if '-' in t})\n",
        "\n",
        "# Some versions of nervaluate return 3 values: (results, results_by_tag, results_per_doc)\n",
        "_eval_out = Evaluator(true_clean, pred_clean, tags=entity_types).evaluate()\n",
        "if isinstance(_eval_out, tuple) and len(_eval_out) >= 2:\n",
        "    results_overall, results_by_tag = _eval_out[0], _eval_out[1]\n",
        "else:\n",
        "    results_overall, results_by_tag = _eval_out, {}\n",
        "\n",
        "rows = []\n",
        "for et in entity_types:\n",
        "    m = results_by_tag.get(et, {}).get('exact', {})\n",
        "    rows.append({\n",
        "        'entity': et,\n",
        "        'TP': m.get('tp', 0),\n",
        "        'FP': m.get('fp', 0),\n",
        "        'FN': m.get('fn', 0),\n",
        "        'precision': m.get('precision', 0.0),\n",
        "        'recall': m.get('recall', 0.0),\n",
        "        'f1': m.get('f1', 0.0),\n",
        "        'mode': 'exact'\n",
        "    })\n",
        "\n",
        "agg = results_overall.get('exact', {})\n",
        "rows.append({'entity': 'OVERALL', 'TP': agg.get('tp', 0), 'FP': agg.get('fp', 0), 'FN': agg.get('fn', 0), 'precision': agg.get('precision', 0.0), 'recall': agg.get('recall', 0.0), 'f1': agg.get('f1', 0.0), 'mode': 'exact'})\n",
        "\n",
        "out_csv = os.path.join('outputs', 'ner_eval_per_type_exact.csv')\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
        "print('Saved:', out_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction:\n",
            " You will label tokens with BIO tags. Output only the tag after each token. One token per line.\n",
            "\n",
            "Text:\n",
            "Transaction details: gasLimit set to 1000000 units by tw_brian740, gasPrice set to 10 Gwei by veronicawood@example.org, contactable at +1-869-341-9301x7005, located at Suite 378, Yolanda Mountain, Burkeberg.\n",
            "\n",
            "Tokens:\n",
            "Transaction\n",
            "Ġdetails\n",
            ":\n",
            "Ġgas\n",
            "Limit\n",
            "Ġset\n",
            "Ġto\n",
            "Ġ\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "Ġunits\n",
            "Ġby\n",
            "Ġtw\n",
            "_b\n",
            "rian\n",
            "7\n",
            "4\n",
            "0\n",
            ",\n",
            "Ġgas\n",
            "Price\n",
            "Ġset\n",
            "Ġto\n",
            "Ġ\n",
            "1\n",
            "0\n",
            "ĠG\n",
            "wei\n",
            "Ġby\n",
            "Ġver\n",
            "onic\n",
            "aw\n",
            "ood\n",
            "@example\n",
            ".org\n",
            ",\n",
            "Ġcontact\n",
            "able\n",
            "Ġat\n",
            "Ġ+\n",
            "1\n",
            "-\n",
            "8\n",
            "6\n",
            "9\n",
            "-\n",
            "3\n",
            "4\n",
            "1\n",
            "-\n",
            "9\n",
            "3\n",
            "0\n",
            "1\n",
            "x\n",
            "7\n",
            "0\n",
            "0\n",
            "5\n",
            ",\n",
            "Ġlocated\n",
            "Ġat\n",
            "ĠSuite\n",
            "Ġ\n",
            "3\n",
            "7\n",
            "8\n",
            ",\n",
            "ĠY\n",
            "ol\n",
            "anda\n",
            "ĠMountain\n",
            ",\n",
            "ĠBurke\n",
            "berg\n",
            ".\n",
            "\n",
            "Labels:\n",
            "\n",
            "Ground truth tags:\n",
            " O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "B-USER_NAME\n",
            "I-USER_NAME\n",
            "I-USER_NAME\n",
            "I-USER_NAME\n",
            "I-USER_NAME\n",
            "I-USER_NAME\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "B-EMAIL\n",
            "I-EMAIL\n",
            "I-EMAIL\n",
            "I-EMAIL\n",
            "I-EMAIL\n",
            "I-EMAIL\n",
            "O\n",
            "O\n",
            "O\n",
            "O\n",
            "B-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "I-PHONE_NUMBER\n",
            "O\n",
            "O\n",
            "O\n",
            "B-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "I-ADDRESS\n",
            "O\n",
            "\n",
            "Model raw output:\n",
            " \n",
            "\n",
            "Extracted BIO tags:\n",
            " \n"
          ]
        }
      ],
      "source": [
        "# Sample inference output\n",
        "idx = 0  # change to inspect another sample\n",
        "\n",
        "prompt = texts[idx]\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad(), torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "    outputs = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "gen_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "pred_tokens = re.findall(r\"^(?:B|I)-[A-Za-z0-9_]+|^O$\", gen_text, flags=re.M)\n",
        "\n",
        "print(\"Instruction:\\n\", prompt)\n",
        "print(\"\\nGround truth tags:\\n\", \"\\n\".join(labels_true[idx]))\n",
        "print(\"\\nModel raw output:\\n\", gen_text)\n",
        "print(\"\\nExtracted BIO tags:\\n\", \"\\n\".join(pred_tokens))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "311-py-env",
      "language": "python",
      "name": "311-py-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
