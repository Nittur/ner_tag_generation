{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate QLoRA NER on Test Set (BIO)\n",
        "\n",
        "This notebook loads the base `Qwen2.5-0.5B` with the trained LoRA adapter, generates BIO tag sequences for test instructions, and computes per-entity TP/FP/FN and precision/recall/F1, plus a seqeval classification report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Torch: 2.5.1+cu121 CUDA: 12.1 is_available: True\n"
          ]
        }
      ],
      "source": [
        "%pip install -q evaluate==0.4.3 seqeval==1.2.2\n",
        "import torch, os, json, re\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "print('Torch:', torch.__version__, 'CUDA:', torch.version.cuda, 'is_available:', torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded base + LoRA from: models/Qwen2.5-0.5B outputs\\qwen25-0.5b-qlora-ner\\lora_adapter\n"
          ]
        }
      ],
      "source": [
        "model_dir = \"models/Qwen2.5-0.5B\"\n",
        "adapter_dir = os.path.join(\"outputs\", \"qwen25-0.5b-qlora-ner\", \"lora_adapter\")\n",
        "\n",
        "tok_path = os.path.join(adapter_dir, \"tokenizer\")\n",
        "if os.path.isdir(tok_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tok_path, use_fast=True, trust_remote_code=True)\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "# Match training\n",
        "try:\n",
        "    tokenizer.padding_side = \"right\"\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "device_map = \"auto\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "try:\n",
        "    base_model.resize_token_embeddings(len(tokenizer))\n",
        "except Exception as e:\n",
        "    print(\"resize_token_embeddings skipped:\", e)\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, adapter_dir, is_trainable=False, ignore_mismatched_sizes=True)\n",
        "model.eval()\n",
        "print(\"Loaded base + LoRA from:\", model_dir, adapter_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['id', 'source', 'text', 'tokens', 'labels', 'instruction', 'response'])\n",
            "Loaded 1 test examples\n"
          ]
        }
      ],
      "source": [
        "test_path = \"outputs/standardized_gretel_pii_masking_en_test.jsonl\"\n",
        "\n",
        "ds = load_dataset(\"json\", data_files=test_path, split=\"train\")\n",
        "\n",
        "print(ds[0].keys())\n",
        "\n",
        "texts = [ex[\"instruction\"] for ex in ds]\n",
        "# Parse per-line BIO tags (one label per line)\n",
        "labels_true = [re.findall(r\"^(?:B|I)-[A-Za-z0-9_]+|^O$\", ex[\"response\"], flags=re.M) for ex in ds]\n",
        "\n",
        "print(\"Loaded\", len(texts), \"test examples\")\n",
        "\n",
        "# Build allowed BIO tag set from TRAIN ground truth to guide decoding\n",
        "train_path = \"outputs/standardized_gretel_pii_masking_en_train.jsonl\"\n",
        "train_ds = load_dataset(\"json\", data_files=train_path, split=\"train\")\n",
        "labels_train = [re.findall(r\"^(?:B|I)-[A-Za-z0-9_]+|^O$\", ex[\"response\"], flags=re.M) for ex in train_ds]\n",
        "allowed_tags = sorted({t for seq in labels_train for t in seq} | {\"O\"})\n",
        "allowed_types = sorted({t.split('-', 1)[1] for t in allowed_tags if '-' in t})\n",
        "\n",
        "# Minimal few-shot example to anchor line-by-line BIO output\n",
        "few_shot_prefix = (\n",
        "    \"Output only one BIO tag per line. Allowed tags: \" + \", \".join(allowed_tags) + \"\\n\\n\"\n",
        "    \"Example:\\n\"\n",
        "    \"Tokens:\\nJohn\\nDoe\\n\\nLabels:\\n\"\n",
        "    + (\"B-NAME\\nI-NAME\\n\" if any(t.startswith(\"B-NAME\") for t in allowed_tags) else \"O\\nO\\n\")\n",
        "    + \"\\n\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal, consistent header (same as training)\n",
        "tag_header = (\n",
        "    \"You will label tokens with BIO tags.\\n\"\n",
        "    \"Output only one tag per line (no extra text).\\n\"\n",
        "    \"Allowed tags: \" + \", \".join(allowed_tags) + \"\\n\\n\"\n",
        ")\n",
        "# Use the same variable the rest of the notebook expects\n",
        "few_shot_prefix = tag_header\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constrained per-line tag projection (argmax over allowed tags)\n",
        "from functools import lru_cache\n",
        "import torch\n",
        "\n",
        "def build_allowed_tag_texts(allowed_types_list):\n",
        "    tags = [\"O\\n\"]\n",
        "    tags += [f\"B-{t}\\n\" for t in allowed_types_list]\n",
        "    tags += [f\"I-{t}\\n\" for t in allowed_types_list]\n",
        "    return tags\n",
        "\n",
        "allowed_tag_texts = build_allowed_tag_texts(allowed_types)\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def _cand_ids_cached(text: str):\n",
        "    return tokenizer(text, add_special_tokens=False).input_ids\n",
        "\n",
        "@torch.inference_mode()\n",
        "def choose_tags_argmax(prompt_text: str, num_lines: int):\n",
        "    # Tokenize prompt once\n",
        "    prompt_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids[0].to(model.device)\n",
        "    chosen_ids = torch.tensor([], dtype=prompt_ids.dtype, device=model.device)\n",
        "    chosen_tags = []\n",
        "\n",
        "    for _ in range(num_lines):\n",
        "        best_score = -1e30\n",
        "        best_tag = \"O\\n\"\n",
        "        best_ids = None\n",
        "\n",
        "        # Run one forward per candidate to score its logprob\n",
        "        for cand in allowed_tag_texts:\n",
        "            cand_ids = torch.tensor(_cand_ids_cached(cand), dtype=prompt_ids.dtype, device=model.device)\n",
        "            # Build full input ids\n",
        "            input_ids = torch.cat([prompt_ids, chosen_ids, cand_ids], dim=0).unsqueeze(0)\n",
        "            out = model(input_ids=input_ids)\n",
        "            logits = out.logits[0]\n",
        "            log_probs = torch.log_softmax(logits, dim=-1)\n",
        "            # Positions: each token prob comes from previous position logits\n",
        "            seq_len = input_ids.shape[-1]\n",
        "            start = seq_len - cand_ids.shape[0]\n",
        "            score = 0.0\n",
        "            pos = start\n",
        "            for k in range(cand_ids.shape[0]):\n",
        "                pred_idx = pos - 1\n",
        "                score += log_probs[pred_idx, cand_ids[k]].item()\n",
        "                pos += 1\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_tag = cand\n",
        "                best_ids = cand_ids\n",
        "\n",
        "        # Commit best tag\n",
        "        chosen_ids = torch.cat([chosen_ids, best_ids], dim=0)\n",
        "        chosen_tags.append(best_tag.strip())\n",
        "\n",
        "    return chosen_tags\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers for robust BIO tag normalization and extraction\n",
        "import string\n",
        "\n",
        "def _levenshtein(a: str, b: str) -> int:\n",
        "    la, lb = len(a), len(b)\n",
        "    if la == 0:\n",
        "        return lb\n",
        "    if lb == 0:\n",
        "        return la\n",
        "    dp = list(range(lb + 1))\n",
        "    for i in range(1, la + 1):\n",
        "        prev = dp[0]\n",
        "        dp[0] = i\n",
        "        ca = a[i - 1]\n",
        "        for j in range(1, lb + 1):\n",
        "            temp = dp[j]\n",
        "            cost = 0 if ca == b[j - 1] else 1\n",
        "            dp[j] = min(\n",
        "                dp[j] + 1,       # deletion\n",
        "                dp[j - 1] + 1,   # insertion\n",
        "                prev + cost      # substitution\n",
        "            )\n",
        "            prev = temp\n",
        "    return dp[lb]\n",
        "\n",
        "def _normalize_type_token(s: str) -> str:\n",
        "    s = s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").upper()\n",
        "    s = re.sub(r\"[^A-Z0-9_]\", \"\", s)\n",
        "    s = re.sub(r\"_+\", \"_\", s)\n",
        "    return s.strip(\"_\")\n",
        "\n",
        "def _best_allowed_type(candidate: str, allowed_types_list) -> str | None:\n",
        "    if not candidate:\n",
        "        return None\n",
        "    cand = _normalize_type_token(candidate)\n",
        "    if cand in allowed_types_list:\n",
        "        return cand\n",
        "    # quick substring heuristic\n",
        "    cand_flat = cand.replace(\"_\", \"\")\n",
        "    for t in allowed_types_list:\n",
        "        tf = t.replace(\"_\", \"\")\n",
        "        if cand_flat in tf or tf in cand_flat:\n",
        "            return t\n",
        "    # Levenshtein-based fallback\n",
        "    best_t, best_d = None, 10**9\n",
        "    for t in allowed_types_list:\n",
        "        d = _levenshtein(cand_flat, t.replace(\"_\", \"\"))\n",
        "        if d < best_d:\n",
        "            best_d, best_t = d, t\n",
        "    # accept if reasonably close\n",
        "    if best_t is not None and best_d <= max(2, len(best_t)//3):\n",
        "        return best_t\n",
        "    return None\n",
        "\n",
        "def extract_bio_tags(text: str, allowed_types_list):\n",
        "    tags = []\n",
        "    for raw in text.splitlines():\n",
        "        ln = raw.strip().strip(\",;:\\u202f \\t\\r\\n\")\n",
        "        if not ln:\n",
        "            continue\n",
        "        # Normalize any separators between prefix and type\n",
        "        m = re.match(r\"^(B|I)[\\s\\-_:]*([A-Za-z0-9_]+)$\", ln, flags=re.IGNORECASE)\n",
        "        if m:\n",
        "            pref = m.group(1).upper()\n",
        "            mapped = _best_allowed_type(m.group(2), allowed_types_list)\n",
        "            tags.append(f\"{pref}-{mapped}\" if mapped else \"O\")\n",
        "            continue\n",
        "        if ln.upper() == \"O\":\n",
        "            tags.append(\"O\")\n",
        "            continue\n",
        "        # If line looks like only a type, coerce to O\n",
        "        tags.append(\"O\")\n",
        "    return tags\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f89ac93e77884e32a3b28255ac1a94a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example prediction:\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "gen_kwargs = dict(\n",
        "    max_new_tokens=1024,  # allow enough lines\n",
        "    min_new_tokens=4,     # enforce at least a few tokens\n",
        "    do_sample=False,\n",
        "    num_beams=1,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "pred_sequences = []\n",
        "\n",
        "for text in tqdm(texts):\n",
        "    prompt = text if text.rstrip().endswith('Labels:') else text + '\\n'\n",
        "    prompt = few_shot_prefix + prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    autocast_ctx = (\n",
        "        torch.amp.autocast(\"cuda\", dtype=torch.bfloat16)\n",
        "        if torch.cuda.is_available() else torch.cpu.amp.autocast(dtype=torch.bfloat16)\n",
        "    )\n",
        "    with torch.no_grad(), autocast_ctx:\n",
        "        outputs = model.generate(**inputs, **gen_kwargs)\n",
        "    gen_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True).strip()\n",
        "    tags = extract_bio_tags(gen_text, allowed_types)\n",
        "    pred_sequences.append(tags)\n",
        "\n",
        "if len(pred_sequences) == 0:\n",
        "    pred_sequences = [[]]\n",
        "\n",
        "print(\"Example prediction:\")\n",
        "print(pred_sequences[0][:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aligned sequences prepared.\n",
            "Example true len, pred len: 81 81\n"
          ]
        }
      ],
      "source": [
        "# Align lengths by truncating/padding with 'O'\n",
        "true_aligned, pred_aligned = [], []\n",
        "for ex, (instr, t, p) in enumerate(zip(texts, labels_true, pred_sequences)):\n",
        "    # Infer the intended token count from the instruction (count lines after 'Tokens:')\n",
        "    tokens_match = re.split(r\"\\bTokens:\\s*\\n\", instr)\n",
        "    expected = None\n",
        "    if len(tokens_match) > 1:\n",
        "        # Up to 'Labels:' marker\n",
        "        token_block = tokens_match[1].split(\"\\n\\nLabels:\", 1)[0]\n",
        "        expected = len([ln for ln in token_block.splitlines() if ln.strip() != \"\"])\n",
        "    L = expected or max(len(t), len(p))\n",
        "    t_adj = (t + [\"O\"] * (L - len(t)))[:L]\n",
        "    p_adj = (p + [\"O\"] * (L - len(p)))[:L]\n",
        "    true_aligned.append(t_adj)\n",
        "    pred_aligned.append(p_adj)\n",
        "\n",
        "print(\"Aligned sequences prepared.\")\n",
        "print(\"Example true len, pred len:\", len(true_aligned[0]), len(pred_aligned[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Saved: outputs\\ner_eval_per_type_exact.csv\n"
          ]
        }
      ],
      "source": [
        "%pip install -q nervaluate pandas\n",
        "import pandas as pd\n",
        "from nervaluate import Evaluator\n",
        "\n",
        "valid = re.compile(r'^(?:(?:B|I)-[A-Za-z0-9_]+|O)$')\n",
        "true_clean = [[t if valid.match(t) else 'O' for t in seq] for seq in true_aligned]\n",
        "pred_clean = [[t if valid.match(t) else 'O' for t in seq] for seq in pred_aligned]\n",
        "\n",
        "entity_types = sorted({t.split('-', 1)[1] for seq in (true_clean + pred_clean) for t in seq if '-' in t})\n",
        "\n",
        "# Some versions of nervaluate return 3 values: (results, results_by_tag, results_per_doc)\n",
        "_eval_out = Evaluator(true_clean, pred_clean, tags=entity_types).evaluate()\n",
        "if isinstance(_eval_out, tuple) and len(_eval_out) >= 2:\n",
        "    results_overall, results_by_tag = _eval_out[0], _eval_out[1]\n",
        "else:\n",
        "    results_overall, results_by_tag = _eval_out, {}\n",
        "\n",
        "rows = []\n",
        "for et in entity_types:\n",
        "    m = results_by_tag.get(et, {}).get('exact', {})\n",
        "    rows.append({\n",
        "        'entity': et,\n",
        "        'TP': m.get('tp', 0),\n",
        "        'FP': m.get('fp', 0),\n",
        "        'FN': m.get('fn', 0),\n",
        "        'precision': m.get('precision', 0.0),\n",
        "        'recall': m.get('recall', 0.0),\n",
        "        'f1': m.get('f1', 0.0),\n",
        "        'mode': 'exact'\n",
        "    })\n",
        "\n",
        "agg = results_overall.get('exact', {})\n",
        "rows.append({'entity': 'OVERALL', 'TP': agg.get('tp', 0), 'FP': agg.get('fp', 0), 'FN': agg.get('fn', 0), 'precision': agg.get('precision', 0.0), 'recall': agg.get('recall', 0.0), 'f1': agg.get('f1', 0.0), 'mode': 'exact'})\n",
        "\n",
        "out_csv = os.path.join('outputs', 'ner_eval_per_type_exact.csv')\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
        "print('Saved:', out_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample inference output\n",
        "idx = 0  # change to inspect another sample\n",
        "\n",
        "prompt = texts[idx]\n",
        "# ensure newline after Labels: to nudge model to start tagging\n",
        "prompt = prompt if prompt.rstrip().endswith('Labels:') else prompt + '\\n'\n",
        "prompt = few_shot_prefix + prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "autocast_ctx = (\n",
        "    torch.amp.autocast(\"cuda\", dtype=torch.bfloat16)\n",
        "    if torch.cuda.is_available() else torch.cpu.amp.autocast(dtype=torch.bfloat16)\n",
        ")\n",
        "with torch.no_grad(), autocast_ctx:\n",
        "    outputs = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "gen_ids = outputs[0][inputs.input_ids.shape[-1]:]\n",
        "gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "if len(gen_text) == 0:\n",
        "    with torch.no_grad(), autocast_ctx:\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    gen_ids = outputs[0][inputs.input_ids.shape[-1]:]\n",
        "    gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "# Try projected tags first\n",
        "proj_L = 0\n",
        "m_tokens = re.split(r\"\\bTokens:\\s*\\n\", prompt)\n",
        "if len(m_tokens) > 1:\n",
        "    token_block = m_tokens[1].split(\"\\n\\nLabels:\", 1)[0]\n",
        "    proj_L = len([ln for ln in token_block.splitlines() if ln.strip() != \"\"])\n",
        "if proj_L > 0:\n",
        "    pred_tokens = choose_tags_argmax(prompt, proj_L)\n",
        "else:\n",
        "    pred_tokens = extract_bio_tags(gen_text, allowed_types)\n",
        "\n",
        "# print(\"Instruction:\\n\", prompt)\n",
        "# print(\"\\nGround truth tags:\\n\", \"\\n\".join(labels_true[idx]))\n",
        "print(\"\\nModel raw output (len=\", len(gen_text), \"):\\n\", gen_text)\n",
        "print(\"\\nGenerated token ids (first 20):\", gen_ids[:20].tolist())\n",
        "print(\"\\nExtracted BIO tags (n=\", len(pred_tokens), \"):\\n\", \"\\n\".join(pred_tokens))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "311-py-env",
      "language": "python",
      "name": "311-py-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
