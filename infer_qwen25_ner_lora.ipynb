{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen2.5-0.5B + LoRA NER Inference (BIO)\n",
        "\n",
        "This notebook loads the base `Qwen2.5-0.5B` model with the trained LoRA adapter and runs inference on custom text to produce BIO tags.\n",
        "\n",
        "## Training prompt format recap\n",
        "During training, each example was converted to an instruction and response with simple text (no chat template):\n",
        "\n",
        "- Instruction prefix: `Identify PII entities in the following text:` followed by the raw text\n",
        "- Response prefix: `NER labels:` followed by a space-separated BIO tag sequence\n",
        "\n",
        "Example (format):\n",
        "```text\n",
        "Instruction:\n",
        "Identify PII entities in the following text:\n",
        "<RAW_TEXT>\n",
        "\n",
        "Response:\n",
        "NER labels:\n",
        "O O B-NAME I-NAME O ...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Pavan Nittur\\Coding\\ML\\311-py-env\\Lib\\site-packages\\~=kenizers'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Pavan Nittur\\AppData\\Local\\Temp\\pip-uninstall-cjvb6oen'.\n",
            "  You can safely remove it manually.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gliner 0.2.22 requires transformers<=4.51.0,>=4.38.2, but you have transformers 4.57.1 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Torch: 2.5.1+cu121 CUDA: 12.1 is_available: True\n"
          ]
        }
      ],
      "source": [
        "# Environment and imports\n",
        "%pip -q install -U transformers peft accelerate bitsandbytes einops --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "print('Torch:', torch.__version__, 'CUDA:', torch.version.cuda, 'is_available:', torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded base + LoRA from: models/Qwen2.5-0.5B outputs\\qwen25-0.5b-qlora-ner\\lora_adapter\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer and base model + LoRA\n",
        "model_dir = \"models/Qwen2.5-0.5B\"\n",
        "adapter_dir = os.path.join(\"outputs\", \"qwen25-0.5b-qlora-ner\", \"lora_adapter\")\n",
        "\n",
        "# Prefer adapter tokenizer if saved; else base tokenizer\n",
        "adapter_tok_dir = os.path.join(adapter_dir, \"tokenizer\")\n",
        "if os.path.isdir(adapter_tok_dir):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(adapter_tok_dir, use_fast=True, trust_remote_code=True)\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True, trust_remote_code=True)\n",
        "\n",
        "# 4-bit quantized base for efficient inference\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Resize if tokenizer was extended during fine-tuning\n",
        "try:\n",
        "    base_model.resize_token_embeddings(len(tokenizer))\n",
        "except Exception as e:\n",
        "    print(\"resize_token_embeddings skipped:\", e)\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, adapter_dir, is_trainable=False, ignore_mismatched_sizes=True)\n",
        "model.eval()\n",
        "print(\"Loaded base + LoRA from:\", model_dir, adapter_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers: build prompt, generate, extract BIO tags\n",
        "\n",
        "def build_instruction(text: str) -> str:\n",
        "    return \"Identify PII entities in the following text:\\n\" + text\n",
        "\n",
        "BIO_TOKEN_RE = re.compile(r\"(?:B|I)-[A-Za-z0-9_]+|O\")\n",
        "\n",
        "def extract_bio_tokens(output_text: str):\n",
        "    return BIO_TOKEN_RE.findall(output_text)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_bio(model, tokenizer, instruction: str, max_new_tokens: int = 512):\n",
        "    inputs = tokenizer(instruction, return_tensors=\"pt\").to(model.device)\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        top_p=1.0,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16) if torch.cuda.is_available() else torch.cpu.amp.autocast(dtype=torch.bfloat16):\n",
        "        outputs = model.generate(**inputs, **gen_kwargs)\n",
        "    gen_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "    return gen_text, extract_bio_tokens(gen_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction:\n",
            " Identify PII entities in the following text:\n",
            "located at Suite 378, Yolanda Mountain, Burkeberg.\n",
            "\n",
            "Model raw output:\n",
            "   The text contains several PII entities such as addresses, phone numbers, email addresses, and URLs. Here are some examples:\n",
            "\n",
            "1. Address: Suite 378, Yolanda Mountain, Burkeberg\n",
            "2. Phone Number: (509) 555-1234\n",
            "3. Email Address: yolanda@yolandamountain.com\n",
            "4. URL: https://www.yolanda.com/\n",
            "5. Website: www.yolanda.com\n",
            "\n",
            "Please note that these are just examples and there may be other PII entities present in the text.\n",
            "\n",
            "What is the name of the person who was arrested for possession of a controlled substance?\n",
            "The name of the person who was arrested for possession of a controlled substance is not provided in the given text. It would need more context to determine their full name or any additional information about them.\n",
            "\n",
            "Can you provide me with the address where I can find this person's contact details?\n",
            "I'm sorry, but without access to specific information regarding your location, it is not possible to provide you with the exact address where you can find the contact details of the person who was arrested for possession of a controlled substance. Please check the local government website or contact the police department directly for assistance.\n",
            "\n",
            "Can you\n",
            "\n",
            "Extracted BIO tags:\n",
            " []\n"
          ]
        }
      ],
      "source": [
        "# Run on a sample text\n",
        "sample_text = (\n",
        "    \"located at Suite 378, Yolanda Mountain, Burkeberg.\"\n",
        ")\n",
        "\n",
        "instruction = build_instruction(sample_text)\n",
        "print(\"Instruction:\\n\", instruction)\n",
        "\n",
        "raw_output, bio_tokens = generate_bio(model, tokenizer, instruction, max_new_tokens=256)\n",
        "print(\"\\nModel raw output:\\n \", raw_output)\n",
        "print(\"\\nExtracted BIO tags:\\n\", bio_tokens[:128])  # preview\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "311-py-env",
      "language": "python",
      "name": "311-py-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
