{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gretel PII Masking (English) — Load local Parquet dataset\n",
        "\n",
        "This notebook loads the Gretel PII Masking English dataset from local Parquet files in the `dataset` folder, shows the dataset structure, and displays one example for quick understanding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading local Parquet dataset...\n",
            "Loaded splits: ['train', 'validation', 'test']\n",
            "- train: 50000 rows; columns=['uid', 'domain', 'document_type', 'document_description', 'entities', 'text']\n",
            "- validation: 5000 rows; columns=['uid', 'domain', 'document_type', 'document_description', 'entities', 'text']\n",
            "- test: 5000 rows; columns=['uid', 'domain', 'document_type', 'document_description', 'entities', 'text']\n",
            "\n",
            "[Sample train] text[:200]: **ADOPTION CERTIFICATE** Issued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12\n",
            "[Sample train] entities (raw) snippet: [{'entity': 'Guernsey', 'types': ['country']}, {'entity': 'Urvashi Jaggi', 'types': ['name']}, {'entity': '2015-07-26', \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['uid', 'domain', 'document_type', 'document_description', 'entities', 'text'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['uid', 'domain', 'document_type', 'document_description', 'entities', 'text'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['uid', 'domain', 'document_type', 'document_description', 'entities', 'text'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load local Parquet files (train/validation/test)\n",
        "# Keep paths simple and local under the `dataset/` folder\n",
        "data_files = {\n",
        "    \"train\": \"dataset/train-00000-of-00001.parquet\",\n",
        "    \"validation\": \"dataset/validation-00000-of-00001.parquet\",\n",
        "    \"test\": \"dataset/test-00000-of-00001.parquet\",\n",
        "}\n",
        "\n",
        "print(\"Loading local Parquet dataset...\")\n",
        "ds = load_dataset(\"parquet\", data_files=data_files)\n",
        "print(\"Loaded splits:\", list(ds.keys()))\n",
        "for name, split in ds.items():\n",
        "    print(f\"- {name}: {split.num_rows} rows; columns={split.column_names}\")\n",
        "\n",
        "# Quick peek at one record for sanity\n",
        "train_sample = ds[\"train\"][0]\n",
        "print(\"\\n[Sample train] text[:200]:\", (train_sample.get(\"text\") or \"\")[:200].replace(\"\\n\", \" \"))\n",
        "print(\"[Sample train] entities (raw) snippet:\", str(train_sample.get(\"entities\"))[:120])\n",
        "\n",
        "# Show dataset object for a quick summary\n",
        "ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UID: 24bb7570700746458ede975349077acb\n",
            "Text preview: **ADOPTION CERTIFICATE** Issued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12\n",
            "Entities (raw): [{'entity': 'Guernsey', 'types': ['country']}, {'entity': 'Urvashi Jaggi', 'types': ['name']}, {'entity': '2015-07-26', 'types': ['date_of_birth']}, {'entity': 'UID-PRWBO4TB', 'types': ['unique_identi\n"
          ]
        }
      ],
      "source": [
        "# Show one example (pretty)\n",
        "example = ds[\"train\"][0]\n",
        "print(\"UID:\", example.get(\"uid\"))\n",
        "print(\"Text preview:\", (example.get(\"text\") or \"\")[:200].replace(\"\\n\", \" \"))\n",
        "print(\"Entities (raw):\", str(example.get(\"entities\"))[:200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalizing entities for split: train ...\n",
            "- train: 50000 rows; with_entities=50000; example: {'text': '**ADOPTION CERTIFICATE**\\nIssued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12-15. Unique identifier: UID-PRWBO4TB.', 'entities': [{'entity': 'Guernsey', 'label': 'COUNTRY'}, {'entity': 'Urvashi Jaggi', 'label': 'NAME'}, {'entity': '2015-07-26', 'label': 'DATE_OF_BIRTH'}, {'entity': 'UID-PRWBO4TB', 'label': 'UNIQUE_IDENTIFIER'}]}\n",
            "Normalizing entities for split: validation ...\n",
            "- validation: 5000 rows; with_entities=5000; example: {'text': 'Patient Name: Alec Stafford, DOB: 1942-08-10, Medical Record Number: P4076208', 'entities': [{'entity': 'Alec', 'label': 'FIRST_NAME'}, {'entity': 'Stafford', 'label': 'LAST_NAME'}, {'entity': '1942-08-10', 'label': 'DATE_OF_BIRTH'}, {'entity': 'P4076208', 'label': 'MEDICAL_RECORD_NUMBER'}]}\n",
            "Normalizing entities for split: test ...\n",
            "- test: 5000 rows; with_entities=5000; example: {'text': 'Transaction details: gasLimit set to 1000000 units by tw_brian740, gasPrice set to 10 Gwei by veronicawood@example.org, contactable at +1-869-341-9301x7005, located at Suite 378, Yolanda Mountain, Burkeberg.', 'entities': [{'entity': 'tw_brian740', 'label': 'USER_NAME'}, {'entity': 'veronicawood@example.org', 'label': 'EMAIL'}, {'entity': '+1-869-341-9301x7005', 'label': 'PHONE_NUMBER'}, {'entity': 'Suite 378, Yolanda Mountain, Burkeberg', 'label': 'ADDRESS'}]}\n",
            "\n",
            "Normalized train example: {'text': '**ADOPTION CERTIFICATE**\\nIssued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12-15. Unique identifier: UID-PRWBO4TB.', 'entities': [{'entity': 'Guernsey', 'label': 'COUNTRY'}, {'entity': 'Urvashi Jaggi', 'label': 'NAME'}, {'entity': '2015-07-26', 'label': 'DATE_OF_BIRTH'}, {'entity': 'UID-PRWBO4TB', 'label': 'UNIQUE_IDENTIFIER'}]}\n"
          ]
        }
      ],
      "source": [
        "# STEP 2 — Normalize entity structure (simple)\n",
        "# - Expect `text` and `entities` per row\n",
        "# - If `entities` is a JSON string, parse to Python list[dict]\n",
        "# - Keep only {\"entity\": <text>, \"label\": <UPPER>}\n",
        "\n",
        "import json\n",
        "import ast\n",
        "\n",
        "\n",
        "def _first_non_empty(seq):\n",
        "    for item in seq:\n",
        "        if item is not None and str(item).strip() != \"\":\n",
        "            return item\n",
        "    return None\n",
        "\n",
        "\n",
        "def parse_entities(entities):\n",
        "    if entities is None:\n",
        "        return []\n",
        "    if isinstance(entities, str):\n",
        "        try:\n",
        "            entities = json.loads(entities)\n",
        "        except Exception:\n",
        "            try:\n",
        "                entities = ast.literal_eval(entities)\n",
        "            except Exception:\n",
        "                return []\n",
        "    if isinstance(entities, dict):\n",
        "        entities = [entities]\n",
        "    if not isinstance(entities, list):\n",
        "        return []\n",
        "    cleaned = []\n",
        "    for e in entities:\n",
        "        if isinstance(e, dict):\n",
        "            ent_text = e.get(\"entity\") or e.get(\"text\") or e.get(\"span\") or e.get(\"value\")\n",
        "            raw_label = e.get(\"label\") or e.get(\"type\") or e.get(\"types\") or e.get(\"tag\") or e.get(\"category\")\n",
        "            if isinstance(raw_label, (list, tuple)):\n",
        "                raw_label = _first_non_empty(raw_label)\n",
        "            if ent_text and raw_label:\n",
        "                cleaned.append({\"entity\": str(ent_text), \"label\": str(raw_label).upper()})\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "normalized = {}\n",
        "for split_name, split in ds.items():\n",
        "    print(f\"Normalizing entities for split: {split_name} ...\")\n",
        "    rows = []\n",
        "    for i in range(split.num_rows):\n",
        "        row = split[i]\n",
        "        text = row.get(\"text\") or \"\"\n",
        "        clean_entities = parse_entities(row.get(\"entities\"))\n",
        "        rows.append({\"text\": text, \"entities\": clean_entities})\n",
        "    normalized[split_name] = rows\n",
        "    has_ents = sum(1 for r in rows if r[\"entities\"])\n",
        "    print(f\"- {split_name}: {len(rows)} rows; with_entities={has_ents}; example:\", rows[0] if rows else None)\n",
        "\n",
        "# Quick peek of one normalized example\n",
        "print(\"\\nNormalized train example:\", normalized.get(\"train\", [])[0] if normalized.get(\"train\") else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building spans for split: train ...\n",
            "- train: 50000 rows; example spans count: 4\n",
            "Building spans for split: validation ...\n",
            "- validation: 5000 rows; example spans count: 4\n",
            "Building spans for split: test ...\n",
            "- test: 5000 rows; example spans count: 4\n",
            "\n",
            "Spanned train example: {'id': '6e85c290-ad9b-4c35-bd80-afb1d80c3e78', 'source': 'gretel_pii_masking_en', 'text': '**ADOPTION CERTIFICATE**\\nIssued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12-15. Unique identifier: UID-PRWBO4TB.', 'clean_text': '**ADOPTION CERTIFICATE** Issued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12-15. Unique identifier: UID-PRWBO4TB.', 'spans': [{'start': 35, 'end': 43, 'label': 'COUNTRY'}, {'start': 103, 'end': 116, 'label': 'NAME'}, {'start': 126, 'end': 136, 'label': 'DATE_OF_BIRTH'}, {'start': 224, 'end': 236, 'label': 'UNIQUE_IDENTIFIER'}]}\n"
          ]
        }
      ],
      "source": [
        "# STEP 3 — Build simple character spans by substring match (case-insensitive)\n",
        "# - Replace newlines for stable indices\n",
        "# - Skip entities we can't find\n",
        "\n",
        "import uuid\n",
        "\n",
        "spanned = {}\n",
        "for split_name, rows in normalized.items():\n",
        "    print(f\"Building spans for split: {split_name} ...\")\n",
        "    out_rows = []\n",
        "    for row in rows:\n",
        "        original_text = row.get(\"text\") or \"\"\n",
        "        clean_text = original_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "\n",
        "        spans = []\n",
        "        for ent in row.get(\"entities\", []):\n",
        "            value = (ent.get(\"entity\") or \"\").strip().strip(\".,;:!?()[]{}\")\n",
        "            label = str(ent.get(\"label\") or \"\").upper()\n",
        "            if not value or not label:\n",
        "                continue\n",
        "            start = clean_text.lower().find(value.lower())\n",
        "            if start == -1:\n",
        "                continue\n",
        "            end = start + len(value)\n",
        "            spans.append({\"start\": start, \"end\": end, \"label\": label})\n",
        "\n",
        "        out_rows.append({\n",
        "            \"id\": str(uuid.uuid4()),\n",
        "            \"source\": \"gretel_pii_masking_en\",\n",
        "            \"text\": original_text,\n",
        "            \"clean_text\": clean_text,\n",
        "            \"spans\": spans,\n",
        "        })\n",
        "    spanned[split_name] = out_rows\n",
        "    print(f\"- {split_name}: {len(out_rows)} rows; example spans count:\", len(out_rows[0][\"spans\"]) if out_rows else 0)\n",
        "\n",
        "# Peek one example\n",
        "print(\"\\nSpanned train example:\", spanned.get(\"train\", [])[0] if spanned.get(\"train\") else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer from: models/Qwen2.5-0.5B\n",
            "Tokenizing and aligning split: train ...\n",
            "- train: 50000 rows; example tokens/labels: (['**', 'ADO', 'PTION', 'ĠCERT', 'IFICATE', '**', 'ĠIss', 'ued', 'Ġby', 'ĠGu'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COUNTRY'])\n",
            "Tokenizing and aligning split: validation ...\n",
            "- validation: 5000 rows; example tokens/labels: (['Patient', 'ĠName', ':', 'ĠAlec', 'ĠStafford', ',', 'ĠDO', 'B', ':', 'Ġ'], ['O', 'O', 'O', 'B-FIRST_NAME', 'B-LAST_NAME', 'O', 'O', 'O', 'O', 'O'])\n",
            "Tokenizing and aligning split: test ...\n",
            "- test: 5000 rows; example tokens/labels: (['Transaction', 'Ġdetails', ':', 'Ġgas', 'Limit', 'Ġset', 'Ġto', 'Ġ', '1', '0'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n",
            "\n",
            "Aligned train example: {'id': '6e85c290-ad9b-4c35-bd80-afb1d80c3e78', 'source': 'gretel_pii_masking_en', 'text': '**ADOPTION CERTIFICATE**\\nIssued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12-15. Unique identifier: UID-PRWBO4TB.', 'tokens': ['**', 'ADO', 'PTION', 'ĠCERT', 'IFICATE', '**', 'ĠIss', 'ued', 'Ġby', 'ĠGu', 'ern', 'sey', 'ĠAdoption', 'ĠAgency', ',', 'Ġthis', 'Ġcertificate', 'Ġconfirms', 'Ġthe', 'Ġadoption', 'Ġof', 'ĠU', 'rv', 'ashi', 'ĠJag', 'gi', ',', 'Ġborn', 'Ġon', 'Ġ', '2', '0', '1', '5', '-', '0', '7', '-', '2', '6', ',', 'Ġby', 'Ġthe', 'Ġadopt', 'ive', 'Ġparents', '.', 'ĠThe', 'Ġadoption', 'Ġwas', 'Ġfinalized', 'Ġon', 'Ġ', '2', '0', '2', '2', '-', '1', '2', '-', '1', '5', '.', 'ĠUnique', 'Ġidentifier', ':', 'ĠUID', '-', 'PR', 'W', 'BO', '4', 'TB', '.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COUNTRY', 'I-COUNTRY', 'I-COUNTRY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'I-NAME', 'I-NAME', 'I-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'B-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'O']}\n"
          ]
        }
      ],
      "source": [
        "# STEP 4 — Tokenize and align spans to BIO labels\n",
        "# - Use the local Qwen tokenizer for consistency with later training\n",
        "# - Align BIO tags from character spans via offset mapping\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer_path = \"models/Qwen2.5-0.5B\"  # or use remote: \"Qwen/Qwen2.5-0.5B\"\n",
        "print(\"Loading tokenizer from:\", tokenizer_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)\n",
        "\n",
        "aligned = {}\n",
        "for split_name, rows in spanned.items():\n",
        "    print(f\"Tokenizing and aligning split: {split_name} ...\")\n",
        "    aligned_rows = []\n",
        "    for row in rows:\n",
        "        text = row.get(\"text\") or \"\"\n",
        "        clean_text = row.get(\"clean_text\", text)\n",
        "        enc = tokenizer(\n",
        "            clean_text,\n",
        "            return_offsets_mapping=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        offsets = enc[\"offset_mapping\"]\n",
        "        tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "\n",
        "        labels = [\"O\"] * len(tokens)\n",
        "        for span in row.get(\"spans\", []):\n",
        "            s, e, lab = span[\"start\"], span[\"end\"], span[\"label\"]\n",
        "            began = False\n",
        "            for ti, (ts, te) in enumerate(offsets):\n",
        "                if te <= s:\n",
        "                    continue\n",
        "                if ts >= e:\n",
        "                    break\n",
        "                if ts < e and te > s:  # overlap between token and span\n",
        "                    labels[ti] = f\"B-{lab}\" if not began else f\"I-{lab}\"\n",
        "                    began = True\n",
        "\n",
        "        aligned_rows.append({\n",
        "            \"id\": row[\"id\"],\n",
        "            \"source\": row[\"source\"],\n",
        "            \"text\": text,\n",
        "            \"tokens\": tokens,\n",
        "            \"labels\": labels,\n",
        "        })\n",
        "    aligned[split_name] = aligned_rows\n",
        "    print(f\"- {split_name}: {len(aligned_rows)} rows; example tokens/labels:\", (aligned_rows[0][\"tokens\"][:10], aligned_rows[0][\"labels\"][:10]) if aligned_rows else None)\n",
        "\n",
        "# Peek one aligned example\n",
        "print(\"\\nAligned train example:\", aligned.get(\"train\", [])[0] if aligned.get(\"train\") else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building instruction/response for split: train ...\n",
            "- train: 50000 rows; example token/label count: (75, 75)\n",
            "Building instruction/response for split: validation ...\n",
            "- validation: 5000 rows; example token/label count: (33, 33)\n",
            "Building instruction/response for split: test ...\n",
            "- test: 5000 rows; example token/label count: (81, 81)\n",
            "\n",
            "Instruction/response train example: {'id': '6e85c290-ad9b-4c35-bd80-afb1d80c3e78', 'source': 'gretel_pii_masking_en', 'text': '**ADOPTION CERTIFICATE**\\nIssued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12-15. Unique identifier: UID-PRWBO4TB.', 'tokens': ['**', 'ADO', 'PTION', 'ĠCERT', 'IFICATE', '**', 'ĠIss', 'ued', 'Ġby', 'ĠGu', 'ern', 'sey', 'ĠAdoption', 'ĠAgency', ',', 'Ġthis', 'Ġcertificate', 'Ġconfirms', 'Ġthe', 'Ġadoption', 'Ġof', 'ĠU', 'rv', 'ashi', 'ĠJag', 'gi', ',', 'Ġborn', 'Ġon', 'Ġ', '2', '0', '1', '5', '-', '0', '7', '-', '2', '6', ',', 'Ġby', 'Ġthe', 'Ġadopt', 'ive', 'Ġparents', '.', 'ĠThe', 'Ġadoption', 'Ġwas', 'Ġfinalized', 'Ġon', 'Ġ', '2', '0', '2', '2', '-', '1', '2', '-', '1', '5', '.', 'ĠUnique', 'Ġidentifier', ':', 'ĠUID', '-', 'PR', 'W', 'BO', '4', 'TB', '.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COUNTRY', 'I-COUNTRY', 'I-COUNTRY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'I-NAME', 'I-NAME', 'I-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'B-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'I-UNIQUE_IDENTIFIER', 'O'], 'instruction': 'You will label tokens with BIO tags. Output only the tag after each token. One token per line.\\n\\nText:\\n**ADOPTION CERTIFICATE**\\nIssued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12-15. Unique identifier: UID-PRWBO4TB.\\n\\nTokens:\\n**\\nADO\\nPTION\\nĠCERT\\nIFICATE\\n**\\nĠIss\\nued\\nĠby\\nĠGu\\nern\\nsey\\nĠAdoption\\nĠAgency\\n,\\nĠthis\\nĠcertificate\\nĠconfirms\\nĠthe\\nĠadoption\\nĠof\\nĠU\\nrv\\nashi\\nĠJag\\ngi\\n,\\nĠborn\\nĠon\\nĠ\\n2\\n0\\n1\\n5\\n-\\n0\\n7\\n-\\n2\\n6\\n,\\nĠby\\nĠthe\\nĠadopt\\nive\\nĠparents\\n.\\nĠThe\\nĠadoption\\nĠwas\\nĠfinalized\\nĠon\\nĠ\\n2\\n0\\n2\\n2\\n-\\n1\\n2\\n-\\n1\\n5\\n.\\nĠUnique\\nĠidentifier\\n:\\nĠUID\\n-\\nPR\\nW\\nBO\\n4\\nTB\\n.\\n\\nLabels:', 'response': 'O\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nB-COUNTRY\\nI-COUNTRY\\nI-COUNTRY\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nB-NAME\\nI-NAME\\nI-NAME\\nI-NAME\\nI-NAME\\nO\\nO\\nO\\nO\\nB-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nB-UNIQUE_IDENTIFIER\\nI-UNIQUE_IDENTIFIER\\nI-UNIQUE_IDENTIFIER\\nI-UNIQUE_IDENTIFIER\\nI-UNIQUE_IDENTIFIER\\nI-UNIQUE_IDENTIFIER\\nI-UNIQUE_IDENTIFIER\\nO'}\n"
          ]
        }
      ],
      "source": [
        "# STEP 5 — Build instruction/response in Token-per-line (CoNLL-lite) format\n",
        "# - Instruction: explains task, shows text and token list (one per line), ends with \"Labels:\" line\n",
        "# - Response: exactly one BIO tag per line (same number as tokens)\n",
        "\n",
        "instr_aligned = {}\n",
        "for split_name, rows in aligned.items():\n",
        "    print(f\"Building instruction/response for split: {split_name} ...\")\n",
        "    out_rows = []\n",
        "    for row in rows:\n",
        "        tokens = row[\"tokens\"]\n",
        "        labels = row[\"labels\"]\n",
        "        token_lines = \"\\n\".join(tokens)\n",
        "        instruction = (\n",
        "            \"You will label tokens with BIO tags. Output only the tag after each token. One token per line.\\n\\n\"\n",
        "            \"Text:\\n\" + row[\"text\"] + \"\\n\\n\"\n",
        "            \"Tokens:\\n\" + token_lines + \"\\n\\n\"\n",
        "            \"Labels:\"\n",
        "        )\n",
        "        response = \"\\n\".join(labels)\n",
        "        out_rows.append({\n",
        "            \"id\": row[\"id\"],\n",
        "            \"source\": row[\"source\"],\n",
        "            \"text\": row[\"text\"],\n",
        "            \"tokens\": tokens,\n",
        "            \"labels\": labels,\n",
        "            \"instruction\": instruction,\n",
        "            \"response\": response,\n",
        "        })\n",
        "    instr_aligned[split_name] = out_rows\n",
        "    print(f\"- {split_name}: {len(out_rows)} rows; example token/label count:\", (len(out_rows[0][\"tokens\"]), len(out_rows[0][\"labels\"])) if out_rows else None)\n",
        "\n",
        "print(\"\\nInstruction/response train example:\", instr_aligned.get(\"train\", [])[0] if instr_aligned.get(\"train\") else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote 50000 rows -> outputs\\standardized_gretel_pii_masking_en_train.jsonl\n",
            "Wrote 5000 rows -> outputs\\standardized_gretel_pii_masking_en_validation.jsonl\n",
            "Wrote 5000 rows -> outputs\\standardized_gretel_pii_masking_en_test.jsonl\n",
            "\n",
            "Preview (first 2 lines of train):\n",
            "{\"id\": \"6e85c290-ad9b-4c35-bd80-afb1d80c3e78\", \"source\": \"gretel_pii_masking_en\", \"text\": \"**ADOPTION CERTIFICATE**\\nIssued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12-15. Unique identifier: UID-PRWBO4TB.\", \"tokens\": [\"**\", \"ADO\", \"PTION\", \"ĠCERT\", \"IFICATE\", \"**\", \"ĠIss\", \"ued\", \"Ġby\", \"ĠGu\", \"ern\", \"sey\", \"ĠAdoption\", \"ĠAgency\", \",\", \"Ġthis\", \"Ġcertificate\", \"Ġconfirms\", \"Ġthe\", \"Ġadoption\", \"Ġof\", \"ĠU\", \"rv\", \"ashi\", \"ĠJag\", \"gi\", \",\", \"Ġborn\", \"Ġon\", \"Ġ\", \"2\", \"0\", \"1\", \"5\", \"-\", \"0\", \"7\", \"-\", \"2\", \"6\", \",\", \"Ġby\", \"Ġthe\", \"Ġadopt\", \"ive\", \"Ġparents\", \".\", \"ĠThe\", \"Ġadoption\", \"Ġwas\", \"Ġfinalized\", \"Ġon\", \"Ġ\", \"2\", \"0\", \"2\", \"2\", \"-\", \"1\", \"2\", \"-\", \"1\", \"5\", \".\", \"ĠUnique\", \"Ġidentifier\", \":\", \"ĠUID\", \"-\", \"PR\", \"W\", \"BO\", \"4\", \"TB\", \".\"], \"labels\": [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-COUNTRY\", \"I-COUNTRY\", \"I-COUNTRY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-NAME\", \"I-NAME\", \"I-NAME\", \"I-NAME\", \"I-NAME\", \"O\", \"O\", \"O\", \"O\", \"B-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-UNIQUE_IDENTIFIER\", \"I-UNIQUE_IDENTIFIER\", \"I-UNIQUE_IDENTIFIER\", \"I-UNIQUE_IDENTIFIER\", \"I-UNIQUE_IDENTIFIER\", \"I-UNIQUE_IDENTIFIER\", \"I-UNIQUE_IDENTIFIER\", \"O\"], \"instruction\": \"You will label tokens with BIO tags. Output only the tag after each token. One token per line.\\n\\nText:\\n**ADOPTION CERTIFICATE**\\nIssued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-12-15. Unique identifier: UID-PRWBO4TB.\\n\\nTokens:\\n**\\nADO\\nPTION\\nĠCERT\\nIFICATE\\n**\\nĠIss\\nued\n",
            "{\"id\": \"1c639fb7-5f91-4775-8efe-6f8480b70311\", \"source\": \"gretel_pii_masking_en\", \"text\": \"**Account Closure Form**\\n\\n**Account Holder:**\\n- **Name:** Zashil Tripathi\\n- **Date of Birth:** 2008-11-18\\n- **SSN:** 819-46-1268\\n\\n**Account Details:**\\n- **Account Number:** G42778036994\\n\\n**Reason for Closure:** Personal\\n\\n**Final Balance:** $5,321.78\", \"tokens\": [\"**\", \"Account\", \"ĠClosure\", \"ĠForm\", \"**\", \"Ġ\", \"Ġ**\", \"Account\", \"ĠHolder\", \":**\", \"Ġ-\", \"Ġ**\", \"Name\", \":**\", \"ĠZ\", \"ash\", \"il\", \"ĠTri\", \"path\", \"i\", \"Ġ-\", \"Ġ**\", \"Date\", \"Ġof\", \"ĠBirth\", \":**\", \"Ġ\", \"2\", \"0\", \"0\", \"8\", \"-\", \"1\", \"1\", \"-\", \"1\", \"8\", \"Ġ-\", \"Ġ**\", \"SS\", \"N\", \":**\", \"Ġ\", \"8\", \"1\", \"9\", \"-\", \"4\", \"6\", \"-\", \"1\", \"2\", \"6\", \"8\", \"Ġ\", \"Ġ**\", \"Account\", \"ĠDetails\", \":**\", \"Ġ-\", \"Ġ**\", \"Account\", \"ĠNumber\", \":**\", \"ĠG\", \"4\", \"2\", \"7\", \"7\", \"8\", \"0\", \"3\", \"6\", \"9\", \"9\", \"4\", \"Ġ\", \"Ġ**\", \"Reason\", \"Ġfor\", \"ĠClosure\", \":**\", \"ĠPersonal\", \"Ġ\", \"Ġ**\", \"Final\", \"ĠBalance\", \":**\", \"Ġ$\", \"5\", \",\", \"3\", \"2\", \"1\", \".\", \"7\", \"8\"], \"labels\": [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-NAME\", \"I-NAME\", \"I-NAME\", \"I-NAME\", \"I-NAME\", \"I-NAME\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"I-DATE_OF_BIRTH\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-SSN\", \"I-SSN\", \"I-SSN\", \"I-SSN\", \"I-SSN\", \"I-SSN\", \"I-SSN\", \"I-SSN\", \"I-SSN\", \"I-SSN\", \"I-SSN\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"I-ACCOUNT_NUMBER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"], \"instruction\": \"You will label tokens with BIO tags. Output only the tag after each token. One token per \n"
          ]
        }
      ],
      "source": [
        "# STEP 6 — Save to JSONL per split\n",
        "# - Compact JSON lines for easy streaming use later\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "for split_name, rows in instr_aligned.items():\n",
        "    out_path = os.path.join(\"outputs\", f\"standardized_gretel_pii_masking_en_{split_name}.jsonl\")\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for row in rows:\n",
        "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "    print(f\"Wrote {len(rows)} rows ->\", out_path)\n",
        "\n",
        "# Preview first 2 lines of train file (if present)\n",
        "preview_path = os.path.join(\"outputs\", \"standardized_gretel_pii_masking_en_train.jsonl\")\n",
        "if os.path.exists(preview_path):\n",
        "    print(\"\\nPreview (first 2 lines of train):\")\n",
        "    with open(preview_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for _ in range(2):\n",
        "            line = f.readline().strip()\n",
        "            if not line:\n",
        "                break\n",
        "            print(line[:2000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "311-py-env",
      "language": "python",
      "name": "311-py-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
