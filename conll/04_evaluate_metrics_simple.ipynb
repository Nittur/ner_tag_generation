{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple NER Evaluation Metrics\n",
        "\n",
        "This notebook evaluates the NER model output in JSON format: `{\"PER\":[...], \"LOC\":[...], \"ORG\":[...], \"MISC\":[...]}`\n",
        "\n",
        "It compares predicted JSON against expected JSON and computes entity-level metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json\n",
        "import ast\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 inference results\n"
          ]
        }
      ],
      "source": [
        "# Load the inference results\n",
        "with open(\"outputs/results/inference_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(results)} inference results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample JSON parsing:\n",
            "Tokens: ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
            "Expected entities: [(['CHINA'], 'PER'), (['JAPAN'], 'LOC')]\n",
            "Predicted entities: []\n"
          ]
        }
      ],
      "source": [
        "def parse_json_output(output_str):\n",
        "    \"\"\"Parse JSON output to extract entities by type\"\"\"\n",
        "    try:\n",
        "        # Try to parse as JSON\n",
        "        data = json.loads(output_str)\n",
        "        \n",
        "        # Convert to list of (entity_text, entity_type)\n",
        "        entities = []\n",
        "        for ent_type in ['PER', 'LOC', 'ORG', 'MISC']:\n",
        "            if ent_type in data and isinstance(data[ent_type], list):\n",
        "                for mention in data[ent_type]:\n",
        "                    if isinstance(mention, list):\n",
        "                        entities.append((mention, ent_type))\n",
        "        return entities\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def extract_tokens_from_input(input_str):\n",
        "    \"\"\"Extract token list from string representation\"\"\"\n",
        "    try:\n",
        "        # Parse the string representation of the list\n",
        "        tokens = ast.literal_eval(input_str)\n",
        "        return tokens\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "print(\"Sample JSON parsing:\")\n",
        "if results:\n",
        "    sample = results[0]\n",
        "    tokens = extract_tokens_from_input(sample['input'])\n",
        "    expected = parse_json_output(sample['expected'])\n",
        "    predicted = parse_json_output(sample['predicted'])\n",
        "    \n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Expected entities: {expected}\")\n",
        "    print(f\"Predicted entities: {predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics calculated successfully!\n"
          ]
        }
      ],
      "source": [
        "def calculate_metrics(results):\n",
        "    \"\"\"Calculate precision, recall, F1 for each entity type from JSON\"\"\"\n",
        "    # Initialize counters\n",
        "    tag_stats = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})\n",
        "    \n",
        "    for result in results:\n",
        "        # Parse expected and predicted JSON\n",
        "        expected_entities = parse_json_output(result['expected'])\n",
        "        predicted_entities = parse_json_output(result['predicted'])\n",
        "        \n",
        "        # Convert to sets for comparison (entity_text, entity_type)\n",
        "        expected_set = {(tuple(ent[0]), ent[1]) for ent in expected_entities}\n",
        "        predicted_set = {(tuple(ent[0]), ent[1]) for ent in predicted_entities}\n",
        "        \n",
        "        # Calculate TP, FP, FN for each entity type\n",
        "        for ent_tuple, ent_type in expected_set:\n",
        "            if (ent_tuple, ent_type) in predicted_set:\n",
        "                # True positive\n",
        "                tag_stats[ent_type]['tp'] += 1\n",
        "            else:\n",
        "                # False negative\n",
        "                tag_stats[ent_type]['fn'] += 1\n",
        "        \n",
        "        for ent_tuple, ent_type in predicted_set:\n",
        "            if (ent_tuple, ent_type) not in expected_set:\n",
        "                # False positive\n",
        "                tag_stats[ent_type]['fp'] += 1\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {}\n",
        "    for tag in ['PER', 'LOC', 'ORG', 'MISC']:\n",
        "        tp = tag_stats[tag]['tp']\n",
        "        fp = tag_stats[tag]['fp']\n",
        "        fn = tag_stats[tag]['fn']\n",
        "        \n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        \n",
        "        metrics[tag] = {\n",
        "            'TP': tp,\n",
        "            'FP': fp,\n",
        "            'FN': fn,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1-Score': f1\n",
        "        }\n",
        "    \n",
        "    # Calculate overall metrics\n",
        "    total_tp = sum(tag_stats[tag]['tp'] for tag in ['PER', 'LOC', 'ORG', 'MISC'])\n",
        "    total_fp = sum(tag_stats[tag]['fp'] for tag in ['PER', 'LOC', 'ORG', 'MISC'])\n",
        "    total_fn = sum(tag_stats[tag]['fn'] for tag in ['PER', 'LOC', 'ORG', 'MISC'])\n",
        "    \n",
        "    overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
        "    overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
        "    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0\n",
        "    \n",
        "    metrics['OVERALL'] = {\n",
        "        'TP': total_tp,\n",
        "        'FP': total_fp,\n",
        "        'FN': total_fn,\n",
        "        'Precision': overall_precision,\n",
        "        'Recall': overall_recall,\n",
        "        'F1-Score': overall_f1\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = calculate_metrics(results)\n",
        "print(\"Metrics calculated successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "NER EVALUATION METRICS (CSV FORMAT)\n",
            "================================================================================\n",
            "\n",
            "Entity,TP,FP,FN,Precision,Recall,F1-Score\n",
            "PER,0,0,1,0.0000,0.0000,0.0000\n",
            "LOC,0,0,1,0.0000,0.0000,0.0000\n",
            "ORG,0,0,0,0.0000,0.0000,0.0000\n",
            "MISC,0,0,0,0.0000,0.0000,0.0000\n",
            "OVERALL,0,0,2,0.0000,0.0000,0.0000\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Print metrics in CSV format\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"NER EVALUATION METRICS (CSV FORMAT)\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nEntity,TP,FP,FN,Precision,Recall,F1-Score\")\n",
        "\n",
        "# Print metrics for each tag and overall\n",
        "for tag in ['PER', 'LOC', 'ORG', 'MISC', 'OVERALL']:\n",
        "    m = metrics[tag]\n",
        "    print(f\"{tag},{m['TP']},{m['FP']},{m['FN']},{m['Precision']:.4f},{m['Recall']:.4f},{m['F1-Score']:.4f}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Metrics saved to: outputs/results/simple_ner_metrics.csv\n",
            "\n",
            "Detailed Metrics Table:\n",
            " Entity  TP  FP  FN  Precision  Recall  F1-Score\n",
            "    PER   0   0   1        0.0     0.0       0.0\n",
            "    LOC   0   0   1        0.0     0.0       0.0\n",
            "    ORG   0   0   0        0.0     0.0       0.0\n",
            "   MISC   0   0   0        0.0     0.0       0.0\n",
            "OVERALL   0   0   2        0.0     0.0       0.0\n"
          ]
        }
      ],
      "source": [
        "# Save metrics to CSV file\n",
        "df = pd.DataFrame.from_dict(metrics, orient='index')\n",
        "df.index.name = 'Entity'\n",
        "df = df.reset_index()\n",
        "\n",
        "# Reorder columns\n",
        "df = df[['Entity', 'TP', 'FP', 'FN', 'Precision', 'Recall', 'F1-Score']]\n",
        "\n",
        "# Save to CSV\n",
        "csv_path = \"outputs/results/simple_ner_metrics.csv\"\n",
        "df.to_csv(csv_path, index=False, float_format='%.4f')\n",
        "print(f\"\\nMetrics saved to: {csv_path}\")\n",
        "\n",
        "# Display the dataframe\n",
        "print(\"\\nDetailed Metrics Table:\")\n",
        "print(df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "EVALUATION SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Results evaluated: 1 samples\n",
            "\n",
            "This evaluation compares JSON format predictions:\n",
            "- Expected: JSON with PER, LOC, ORG, MISC keys\n",
            "- Predicted: Model-generated JSON\n",
            "- Metrics: Entity-level precision, recall, F1\n"
          ]
        }
      ],
      "source": [
        "# Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nResults evaluated: {len(results)} samples\")\n",
        "print(\"\\nThis evaluation compares JSON format predictions:\")\n",
        "print(\"- Expected: JSON with PER, LOC, ORG, MISC keys\")\n",
        "print(\"- Predicted: Model-generated JSON\")\n",
        "print(\"- Metrics: Entity-level precision, recall, F1\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "311-py-env",
      "language": "python",
      "name": "311-py-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
