{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference with Fine-tuned NER Model\n",
        "\n",
        "This notebook demonstrates how to use the fine-tuned Qwen model for NER label generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load base model and LoRA weights\n",
        "base_model_path = \"models/Qwen2.5-0.5B-Instruct\"\n",
        "adapter_path = \"outputs/final_model\"\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
        "\n",
        "# Load base model without quantization\n",
        "print(\"Loading base model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16  # Use fp16 for efficiency\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "print(\"Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(model, adapter_path)\n",
        "\n",
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference function\n",
        "def generate_ner_labels(sentence, max_length=512):\n",
        "    \"\"\"Generate NER labels for a given sentence\"\"\"\n",
        "    \n",
        "    # Prepare the prompt\n",
        "    instruction = \"Given the following sentence, identify and label each word with its named entity tag (PER for person, LOC for location, ORG for organization, MISC for miscellaneous, or O for no entity).\"\n",
        "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{sentence}\\n\\n### Response:\\n\"\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract only the response part\n",
        "    response_marker = \"### Response:\\n\"\n",
        "    if response_marker in generated_text:\n",
        "        response = generated_text.split(response_marker)[-1].strip()\n",
        "    else:\n",
        "        response = generated_text\n",
        "    \n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on sample sentences\n",
        "test_sentences = [\n",
        "    \"Barack Obama was born in Hawaii.\",\n",
        "    \"Microsoft announced a new partnership with OpenAI in San Francisco.\",\n",
        "    \"The Eiffel Tower is located in Paris, France.\",\n",
        "    \"John Smith works at Google headquarters in Mountain View.\",\n",
        "    \"The United Nations conference will be held in New York next month.\"\n",
        "]\n",
        "\n",
        "print(\"Testing on sample sentences:\\n\")\n",
        "for sentence in test_sentences:\n",
        "    print(f\"Input: {sentence}\")\n",
        "    labels = generate_ner_labels(sentence)\n",
        "    print(f\"Output: {labels}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "# Load test data\n",
        "with open(\"outputs/data/test_instruction_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# Test on a subset\n",
        "num_samples = 10\n",
        "print(f\"\\nEvaluating on {num_samples} test samples:\\n\")\n",
        "\n",
        "results = []\n",
        "for i in range(num_samples):\n",
        "    sample = test_data[i]\n",
        "    input_text = sample['input']\n",
        "    expected_output = sample['output']\n",
        "    \n",
        "    # Generate prediction\n",
        "    predicted_output = generate_ner_labels(input_text)\n",
        "    \n",
        "    results.append({\n",
        "        'input': input_text,\n",
        "        'expected': expected_output,\n",
        "        'predicted': predicted_output\n",
        "    })\n",
        "    \n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Input: {input_text}\")\n",
        "    print(f\"Expected: {expected_output[:100]}...\")\n",
        "    print(f\"Predicted: {predicted_output[:100]}...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Save results\n",
        "with open(\"outputs/results/inference_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nResults saved to outputs/results/inference_results.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple evaluation metrics\n",
        "def parse_output(output_str):\n",
        "    \"\"\"Parse the output string into a dictionary of word: tag pairs\"\"\"\n",
        "    pairs = {}\n",
        "    parts = output_str.split(\", \")\n",
        "    for part in parts:\n",
        "        if \": \" in part:\n",
        "            word, tag = part.split(\": \", 1)\n",
        "            pairs[word.strip()] = tag.strip()\n",
        "    return pairs\n",
        "\n",
        "# Calculate exact match accuracy\n",
        "exact_matches = 0\n",
        "for result in results:\n",
        "    if result['expected'].strip() == result['predicted'].strip():\n",
        "        exact_matches += 1\n",
        "\n",
        "print(f\"\\nExact match accuracy: {exact_matches}/{len(results)} = {exact_matches/len(results)*100:.2f}%\")\n",
        "\n",
        "# Calculate tag-level accuracy (approximate)\n",
        "total_tags = 0\n",
        "correct_tags = 0\n",
        "\n",
        "for result in results:\n",
        "    expected_pairs = parse_output(result['expected'])\n",
        "    predicted_pairs = parse_output(result['predicted'])\n",
        "    \n",
        "    for word, expected_tag in expected_pairs.items():\n",
        "        total_tags += 1\n",
        "        if word in predicted_pairs and predicted_pairs[word] == expected_tag:\n",
        "            correct_tags += 1\n",
        "\n",
        "if total_tags > 0:\n",
        "    print(f\"Tag-level accuracy (approximate): {correct_tags}/{total_tags} = {correct_tags/total_tags*100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
