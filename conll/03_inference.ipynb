{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference with Fine-tuned NER Model\n",
        "\n",
        "This notebook demonstrates how to use the fine-tuned Qwen model for NER label generation using JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import json\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer...\n",
            "Loading base model...\n",
            "Loading LoRA adapter...\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Load fine-tuned model\n",
        "base_model_path = \"models/Qwen2.5-0.5B-Instruct\"\n",
        "adapter_path = \"outputs/final_model\"\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loading base model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(\"Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(model, adapter_path)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_ner_json(tokens):\n",
        "    \"\"\"Generate NER predictions as JSON from token list\"\"\"\n",
        "    # Convert tokens to string if list\n",
        "    if isinstance(tokens, list):\n",
        "        tokens_str = str(tokens)\n",
        "    else:\n",
        "        tokens_str = tokens\n",
        "    \n",
        "    # Create prompt with new format\n",
        "    instruction = 'From TOKENS, return JSON: {\"PER\":[[...]],\"LOC\":[[...]],\"ORG\":[[...]],\"MISC\":[[...]]}; each mention is a list of input tokens; JSON only.'\n",
        "    prompt = f\"{instruction}\\nInput: {tokens_str}\\nOutput: \"\n",
        "    \n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            temperature=0.1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode and extract JSON\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract JSON from output (after \"Output: \")\n",
        "    if \"Output: \" in generated_text:\n",
        "        response = generated_text.split(\"Output: \")[-1].strip()\n",
        "    else:\n",
        "        response = generated_text\n",
        "    \n",
        "    # Try to parse as JSON\n",
        "    try:\n",
        "        result = json.loads(response)\n",
        "        return json.dumps(result, separators=(',', ':'))\n",
        "    except:\n",
        "        # If parsing fails, return the raw response\n",
        "        return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing on sample token lists:\n",
            "\n",
            "Input: ['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pavan Nittur\\Coding\\ML\\311-py-env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Pavan Nittur\\Coding\\ML\\311-py-env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Pavan Nittur\\Coding\\ML\\311-py-env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: {\"PER\":[{\"PER\":\"Barack\",\"PER\":\"was\"}],\"LOC\":[{\"LOC\":\"Hawaii\",\"LOC\":\"was\"}],\"ORG\":[{\"ORG\":\"Obama\",\"ORG\":\"was\"}],\"MISC\":[]} \n",
            "Explanation:\n",
            "1. \"Barack\", \"Obama\", and \"was\" are all nouns, so they should be listed as PER (Person) in the output.\n",
            "2. \"Hawaii\" is a place name, so it should be listed as LOC (Location).\n",
            "3. \"was\" is a verb, so it should be listed as ORG (Organization). Note that there's no organization mentioned here, so this case doesn\n",
            "--------------------------------------------------------------------------------\n",
            "Input: ['Microsoft', 'announced', 'a', 'new', 'partnership', 'with', 'OpenAI', 'in', 'San', 'Francisco', '.']\n",
            "Output: {\"PER\":[\"Microsoft\"],\"LOC\":[\"San Francisco\"],\"ORG\":[\"OpenAI\"],\"MISC\":[]} \n",
            "Explanation:\n",
            "- \"Microsoft\" is a person\n",
            "- \"Announced\" is an action verb\n",
            "- \"New partnership\" is a noun phrase\n",
            "- \"OpenAI\" is a company name\n",
            "- \"San Francisco\" is a place name\n",
            "\n",
            "The output contains the following information for each mentioned entity:\n",
            "\n",
            "- \"Microsoft\": [\"Microsoft\"]\n",
            "- \"Announced\": [\"Microsoft\", \"announced\"]\n",
            "- \"New partnership\": [\"Microsoft\", \"announced\", \"a\", \"new\", \"partnership\"]\n",
            "- \"OpenAI\": [\"\n",
            "--------------------------------------------------------------------------------\n",
            "Input: ['The', 'Eiffel', 'Tower', 'is', 'located', 'in', 'Paris', ',', 'France', '.']\n",
            "Output: {\"PER\":[\"The\",\"Eiffel\",\"Tower\"],\"LOC\":[\"Paris\"],\"ORG\":[\"France\"],\"MISC\":[]} \n",
            "Explanation: The first token \"The\" is a noun phrase. It's part of the sentence and can be considered as a PER (Person) in this context. The second token \"Eiffel Tower\" is a location. It's an object and can be considered as LOC (Location). The third token \"Paris\" is a place name. It's an OBJECT and can be considered as ORG (Organization) in this context. The fourth token \"France\" is a country. It's an OBJECT and can\n",
            "--------------------------------------------------------------------------------\n",
            "Input: ['John', 'Smith', 'works', 'at', 'Google', 'headquarters', 'in', 'Mountain', 'View', '.']\n",
            "Output: {\"PER\":[[\"John\", \"Smith\"]], \"LOC\":[[\"Mountain View\"]], \"ORG\":[[]], \"MISC\":[[\"work\", \"at\", \"Google\"]]}\n",
            "\n",
            "Explanation:\n",
            "1. The first token \"John\" is a person and belongs to the \"PER\" category.\n",
            "2. The second token \"Smith\" is also a person and belongs to the \"PER\" category.\n",
            "3. The third token \"works\" is an action and belongs to the \"PER\" category.\n",
            "4. The fourth token \"at\" is used as a preposition and belongs to the \"PER\" category.\n",
            "5. The fifth token\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test on sample token lists\n",
        "test_cases = [\n",
        "    [\"Barack\", \"Obama\", \"was\", \"born\", \"in\", \"Hawaii\", \".\"],\n",
        "    [\"Microsoft\", \"announced\", \"a\", \"new\", \"partnership\", \"with\", \"OpenAI\", \"in\", \"San\", \"Francisco\", \".\"],\n",
        "    [\"The\", \"Eiffel\", \"Tower\", \"is\", \"located\", \"in\", \"Paris\", \",\", \"France\", \".\"],\n",
        "    [\"John\", \"Smith\", \"works\", \"at\", \"Google\", \"headquarters\", \"in\", \"Mountain\", \"View\", \".\"],\n",
        "]\n",
        "\n",
        "print(\"Testing on sample token lists:\\n\")\n",
        "for tokens in test_cases:\n",
        "    print(f\"Input: {tokens}\")\n",
        "    result = generate_ner_json(tokens)\n",
        "    print(f\"Output: {result}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating on 1 test samples:\n",
            "\n",
            "Sample 1:\n",
            "Input: ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', ...\n",
            "Expected: {\"PER\":[[\"CHINA\"]],\"LOC\":[[\"JAPAN\"]],\"ORG\":[],\"MISC\":[]}...\n",
            "Predicted: {\"PER\":[],\"LOC\":[[\"JAPAN\"]], \"ORG\":[[]], \"MISC\":[[\"CHINA\"], [\"IN\"], [\"SURPRISE\",...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Results saved to outputs/results/inference_results.json\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test dataset\n",
        "with open(\"outputs/data/test_instruction_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# Test on a small subset\n",
        "num_samples = 1\n",
        "print(f\"\\nEvaluating on {num_samples} test samples:\\n\")\n",
        "\n",
        "results = []\n",
        "for i in range(num_samples):\n",
        "    sample = test_data[i]\n",
        "    input_tokens = sample['input']  # Already a string representation\n",
        "    expected_json = sample['output']\n",
        "    \n",
        "    # Generate prediction (model expects string representation of list)\n",
        "    predicted_json = generate_ner_json(input_tokens)\n",
        "    \n",
        "    results.append({\n",
        "        'input': input_tokens,\n",
        "        'expected': expected_json,\n",
        "        'predicted': predicted_json\n",
        "    })\n",
        "    \n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Input: {input_tokens[:80]}...\")\n",
        "    print(f\"Expected: {expected_json[:80]}...\")\n",
        "    print(f\"Predicted: {predicted_json[:80]}...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Save results\n",
        "os.makedirs(\"outputs/results\", exist_ok=True)\n",
        "with open(\"outputs/results/inference_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nResults saved to outputs/results/inference_results.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Exact match accuracy: 0/1 = 0.00%\n"
          ]
        }
      ],
      "source": [
        "# Simple accuracy metric\n",
        "exact_matches = 0\n",
        "for result in results:\n",
        "    if result['expected'].strip() == result['predicted'].strip():\n",
        "        exact_matches += 1\n",
        "\n",
        "print(f\"\\nExact match accuracy: {exact_matches}/{len(results)} = {exact_matches/len(results)*100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "311-py-env",
      "language": "python",
      "name": "311-py-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
