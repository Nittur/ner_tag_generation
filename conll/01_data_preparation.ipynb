{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation for NER Label Generation\n",
        "\n",
        "This notebook prepares the CoNLL-2003 dataset for instruction tuning. We'll convert the traditional NER tags into a text generation format suitable for training Qwen 0.5B model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install datasets transformers peft accelerate bitsandbytes trl -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json\n",
        "import os\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded: Train=14041, Val=3250, Test=3453\n",
            "Tag names: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
            "Sample tokens: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n"
          ]
        }
      ],
      "source": [
        "# Load CoNLL-2003 dataset\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "# Get tag names mapping (B-PER, B-ORG, etc.)\n",
        "tag_names = dataset['train'].features['ner_tags'].feature.names\n",
        "\n",
        "print(f\"Dataset loaded: Train={len(dataset['train'])}, Val={len(dataset['validation'])}, Test={len(dataset['test'])}\")\n",
        "print(f\"Tag names: {tag_names}\")\n",
        "print(f\"Sample tokens: {dataset['train'][0]['tokens']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example output:\n",
            "Input: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "Output: {\"PER\":[],\"LOC\":[],\"ORG\":[[\"EU\"]],\"MISC\":[[\"German\"],[\"British\"]]}\n",
            "\n",
            "Result dict: {'PER': [], 'LOC': [], 'ORG': [['EU']], 'MISC': [['German'], ['British']]}\n"
          ]
        }
      ],
      "source": [
        "# Show example: Convert BIO tags to JSON format\n",
        "sample = dataset['train'][0]\n",
        "tokens = sample['tokens']\n",
        "ner_tags = sample['ner_tags']\n",
        "\n",
        "# Convert tag indices to tag names (e.g., 0->O, 3->B-ORG)\n",
        "tags = [tag_names[tag_id] for tag_id in ner_tags]\n",
        "\n",
        "# Initialize result structure\n",
        "result = {\"PER\": [], \"LOC\": [], \"ORG\": [], \"MISC\": []}\n",
        "current_entity = None\n",
        "current_type = None\n",
        "\n",
        "# Process tokens to extract entities\n",
        "for token, tag in zip(tokens, tags):\n",
        "    if tag.startswith('B-') or tag.startswith('I-'):\n",
        "        # Extract entity type (PER, LOC, ORG, or MISC)\n",
        "        entity_type = tag.split('-')[1]\n",
        "        \n",
        "        if tag.startswith('B-'):\n",
        "            # Beginning: save previous entity and start new one\n",
        "            if current_entity:\n",
        "                result[current_type].append(current_entity)\n",
        "            current_type = entity_type\n",
        "            current_entity = [token]\n",
        "        elif tag.startswith('I-') and current_type == entity_type:\n",
        "            # Inside: continue building current entity\n",
        "            current_entity.append(token)\n",
        "    else:\n",
        "        # Outside: end current entity\n",
        "        if current_entity:\n",
        "            result[current_type].append(current_entity)\n",
        "            current_entity = None\n",
        "            current_type = None\n",
        "\n",
        "# Handle any remaining entity at the end\n",
        "if current_entity:\n",
        "    result[current_type].append(current_entity)\n",
        "\n",
        "# Create instruction\n",
        "instruction = 'From TOKENS, return JSON: {\"PER\":[[...]],\"LOC\":[[...]],\"ORG\":[[...]],\"MISC\":[[...]]}; each mention is a list of input tokens; JSON only.'\n",
        "input_text = str(tokens)\n",
        "output_text = json.dumps(result, separators=(',', ':'))\n",
        "\n",
        "print(\"Example output:\")\n",
        "print(f\"Input: {input_text}\")\n",
        "print(f\"Output: {output_text}\")\n",
        "print(f\"\\nResult dict: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing training data...\n",
            "Processing validation data...\n",
            "Processing test data...\n",
            "\n",
            "Processed: Train=14041, Val=3250, Test=3453\n"
          ]
        }
      ],
      "source": [
        "# Define instruction for the model\n",
        "instruction = 'From TOKENS, return JSON: {\"PER\":[[...]],\"LOC\":[[...]],\"ORG\":[[...]],\"MISC\":[[...]]}; each mention is a list of input tokens; JSON only.'\n",
        "\n",
        "# Helper function: Convert BIO tags to JSON format\n",
        "def bio_to_json(tokens, ner_tags):\n",
        "    \"\"\"Convert BIO tags to JSON with entity mentions.\"\"\"\n",
        "    # Convert tag indices to tag names\n",
        "    tags = [tag_names[tag_id] for tag_id in ner_tags]\n",
        "    \n",
        "    # Initialize result structure\n",
        "    result = {\"PER\": [], \"LOC\": [], \"ORG\": [], \"MISC\": []}\n",
        "    current_entity = None\n",
        "    current_type = None\n",
        "    \n",
        "    # Process each token\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        if tag.startswith('B-') or tag.startswith('I-'):\n",
        "            # Extract entity type (PER, LOC, ORG, or MISC)\n",
        "            entity_type = tag.split('-')[1]\n",
        "            \n",
        "            if tag.startswith('B-'):\n",
        "                # Beginning tag: save previous and start new entity\n",
        "                if current_entity:\n",
        "                    result[current_type].append(current_entity)\n",
        "                current_type = entity_type\n",
        "                current_entity = [token]\n",
        "            elif tag.startswith('I-') and current_type == entity_type:\n",
        "                # Inside tag: continue current entity\n",
        "                current_entity.append(token)\n",
        "        else:\n",
        "            # Outside tag: end current entity\n",
        "            if current_entity:\n",
        "                result[current_type].append(current_entity)\n",
        "                current_entity = None\n",
        "                current_type = None\n",
        "    \n",
        "    # Handle remaining entity at the end\n",
        "    if current_entity:\n",
        "        result[current_type].append(current_entity)\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Process all datasets\n",
        "train_data = []\n",
        "val_data = []\n",
        "test_data = []\n",
        "\n",
        "print(\"Processing training data...\")\n",
        "for example in dataset['train']:\n",
        "    result = bio_to_json(example['tokens'], example['ner_tags'])\n",
        "    train_data.append({\n",
        "        'instruction': instruction,\n",
        "        'input': str(example['tokens']),\n",
        "        'output': json.dumps(result, separators=(',', ':')),\n",
        "        'text': f\"{instruction}\\nInput: {str(example['tokens'])}\\nOutput: {json.dumps(result, separators=(',', ':'))}\"\n",
        "    })\n",
        "\n",
        "print(\"Processing validation data...\")\n",
        "for example in dataset['validation']:\n",
        "    result = bio_to_json(example['tokens'], example['ner_tags'])\n",
        "    val_data.append({\n",
        "        'instruction': instruction,\n",
        "        'input': str(example['tokens']),\n",
        "        'output': json.dumps(result, separators=(',', ':')),\n",
        "        'text': f\"{instruction}\\nInput: {str(example['tokens'])}\\nOutput: {json.dumps(result, separators=(',', ':'))}\"\n",
        "    })\n",
        "\n",
        "print(\"Processing test data...\")\n",
        "for example in dataset['test']:\n",
        "    result = bio_to_json(example['tokens'], example['ner_tags'])\n",
        "    test_data.append({\n",
        "        'instruction': instruction,\n",
        "        'input': str(example['tokens']),\n",
        "        'output': json.dumps(result, separators=(',', ':')),\n",
        "        'text': f\"{instruction}\\nInput: {str(example['tokens'])}\\nOutput: {json.dumps(result, separators=(',', ':'))}\"\n",
        "    })\n",
        "\n",
        "print(f\"\\nProcessed: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to outputs/data/\n"
          ]
        }
      ],
      "source": [
        "# Save processed data as JSON files\n",
        "os.makedirs(\"outputs/data\", exist_ok=True)\n",
        "\n",
        "# Save train data\n",
        "with open(\"outputs/data/train_instruction_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Save validation data\n",
        "with open(\"outputs/data/val_instruction_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Save test data\n",
        "with open(\"outputs/data/test_instruction_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(test_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Data saved to outputs/data/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample 5735:\n",
            "From TOKENS, return JSON: {\"PER\":[[...]],\"LOC\":[[...]],\"ORG\":[[...]],\"MISC\":[[...]]}; each mention is a list of input tokens; JSON only.\n",
            "Input: ['0-0', '.']\n",
            "Output: {\"PER\":[],\"LOC\":[],\"ORG\":[],\"MISC\":...\n"
          ]
        }
      ],
      "source": [
        "# Show sample from training data\n",
        "import random\n",
        "sample_idx = random.randint(0, len(train_data) - 1)\n",
        "print(f\"\\nSample {sample_idx}:\")\n",
        "print(train_data[sample_idx]['text'][:200] + \"...\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "311-py-env",
      "language": "python",
      "name": "311-py-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
