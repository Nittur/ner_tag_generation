{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation for NER Label Generation\n",
        "\n",
        "This notebook prepares the CoNLL-2003 dataset for instruction tuning. We'll convert the traditional NER tags into a text generation format suitable for training Qwen 0.5B model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install datasets transformers peft accelerate bitsandbytes trl -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CoNLL-2003 dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "753c695cf93f48f088ce62ad674b8490",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "conll2003.py: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pavan Nittur\\Coding\\ML\\311-py-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Pavan Nittur\\.cache\\huggingface\\hub\\datasets--conll2003. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76bcaffddb9e4c8dad761ce66033ebcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
            "Train size: 14041\n",
            "Validation size: 3250\n",
            "Test size: 3453\n",
            "\n",
            "Sample from training set:\n",
            "Tokens: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "NER tags: [3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
            "\n",
            "NER tag names: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ],
      "source": [
        "# Load CoNLL-2003 dataset\n",
        "print(\"Loading CoNLL-2003 dataset...\")\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "# Check the structure\n",
        "print(f\"Dataset splits: {dataset.keys()}\")\n",
        "print(f\"Train size: {len(dataset['train'])}\")\n",
        "print(f\"Validation size: {len(dataset['validation'])}\")\n",
        "print(f\"Test size: {len(dataset['test'])}\")\n",
        "\n",
        "# Look at a sample\n",
        "print(\"\\nSample from training set:\")\n",
        "sample = dataset['train'][0]\n",
        "print(f\"Tokens: {sample['tokens']}\")\n",
        "print(f\"NER tags: {sample['ner_tags']}\")\n",
        "\n",
        "# Tag mapping\n",
        "tag_names = dataset['train'].features['ner_tags'].feature.names\n",
        "print(f\"\\nNER tag names: {tag_names}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed sample:\n",
            "Instruction: Given the following sentence, identify and label each word with its named entity tag (PER for person, LOC for location, ORG for organization, MISC for miscellaneous, or O for no entity).\n",
            "Input: EU rejects German call to boycott British lamb .\n",
            "Output: EU: B-ORG, rejects: O, German: B-MISC, call: O, to: O, boycott: O, British: B-MISC, lamb: O, .: O...\n"
          ]
        }
      ],
      "source": [
        "# Create instruction format data\n",
        "def create_instruction_format(example):\n",
        "    \"\"\"\n",
        "    Convert tokens and NER tags to instruction format.\n",
        "    Input: sentence\n",
        "    Output: word: tag, word: tag, ...\n",
        "    \"\"\"\n",
        "    tokens = example['tokens']\n",
        "    ner_tags = example['ner_tags']\n",
        "    \n",
        "    # Convert tag indices to tag names\n",
        "    tags = [tag_names[tag_id] for tag_id in ner_tags]\n",
        "    \n",
        "    # Create the input sentence\n",
        "    input_text = \" \".join(tokens)\n",
        "    \n",
        "    # Create the output format\n",
        "    output_pairs = []\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        output_pairs.append(f\"{token}: {tag}\")\n",
        "    output_text = \", \".join(output_pairs)\n",
        "    \n",
        "    # Create instruction format\n",
        "    instruction = \"Given the following sentence, identify and label each word with its named entity tag (PER for person, LOC for location, ORG for organization, MISC for miscellaneous, or O for no entity).\"\n",
        "    \n",
        "    return {\n",
        "        'instruction': instruction,\n",
        "        'input': input_text,\n",
        "        'output': output_text,\n",
        "        'text': f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output_text}\"\n",
        "    }\n",
        "\n",
        "# Process a sample\n",
        "sample_processed = create_instruction_format(dataset['train'][0])\n",
        "print(\"Processed sample:\")\n",
        "print(f\"Instruction: {sample_processed['instruction']}\")\n",
        "print(f\"Input: {sample_processed['input']}\")\n",
        "print(f\"Output: {sample_processed['output'][:100]}...\")  # Truncate for display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing training data...\n",
            "Processing validation data...\n",
            "Processing test data...\n",
            "\n",
            "Processed data sizes:\n",
            "Train: 14041\n",
            "Validation: 3250\n",
            "Test: 3453\n"
          ]
        }
      ],
      "source": [
        "# Process all splits\n",
        "train_data = []\n",
        "val_data = []\n",
        "test_data = []\n",
        "\n",
        "print(\"Processing training data...\")\n",
        "for example in dataset['train']:\n",
        "    train_data.append(create_instruction_format(example))\n",
        "\n",
        "print(\"Processing validation data...\")\n",
        "for example in dataset['validation']:\n",
        "    val_data.append(create_instruction_format(example))\n",
        "\n",
        "print(\"Processing test data...\")\n",
        "for example in dataset['test']:\n",
        "    test_data.append(create_instruction_format(example))\n",
        "\n",
        "print(f\"\\nProcessed data sizes:\")\n",
        "print(f\"Train: {len(train_data)}\")\n",
        "print(f\"Validation: {len(val_data)}\")\n",
        "print(f\"Test: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to outputs/data/\n"
          ]
        }
      ],
      "source": [
        "# Save processed data as JSON files\n",
        "os.makedirs(\"outputs/data\", exist_ok=True)\n",
        "\n",
        "# Save train data\n",
        "with open(\"outputs/data/train_instruction_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Save validation data\n",
        "with open(\"outputs/data/val_instruction_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Save test data\n",
        "with open(\"outputs/data/test_instruction_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(test_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Data saved to outputs/data/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tag distribution:\n",
            "O: 250660 (83.16%)\n",
            "B-LOC: 10645 (3.53%)\n",
            "B-PER: 10059 (3.34%)\n",
            "B-ORG: 9323 (3.09%)\n",
            "I-PER: 6991 (2.32%)\n",
            "I-ORG: 5290 (1.76%)\n",
            "B-MISC: 5062 (1.68%)\n",
            "I-MISC: 1717 (0.57%)\n",
            "I-LOC: 1671 (0.55%)\n",
            "\n",
            "Sequence length statistics:\n",
            "Min length: 1\n",
            "Max length: 113\n",
            "Average length: 14.50\n",
            "95th percentile: 37\n"
          ]
        }
      ],
      "source": [
        "# Analyze the data\n",
        "# Check distribution of tags\n",
        "all_tags = []\n",
        "for split in [dataset['train'], dataset['validation'], dataset['test']]:\n",
        "    for example in split:\n",
        "        tags = [tag_names[tag_id] for tag_id in example['ner_tags']]\n",
        "        all_tags.extend(tags)\n",
        "\n",
        "tag_counter = Counter(all_tags)\n",
        "print(\"Tag distribution:\")\n",
        "for tag, count in tag_counter.most_common():\n",
        "    print(f\"{tag}: {count} ({count/len(all_tags)*100:.2f}%)\")\n",
        "\n",
        "# Check sequence lengths\n",
        "lengths = []\n",
        "for example in dataset['train']:\n",
        "    lengths.append(len(example['tokens']))\n",
        "\n",
        "print(f\"\\nSequence length statistics:\")\n",
        "print(f\"Min length: {min(lengths)}\")\n",
        "print(f\"Max length: {max(lengths)}\")\n",
        "print(f\"Average length: {sum(lengths)/len(lengths):.2f}\")\n",
        "print(f\"95th percentile: {sorted(lengths)[int(len(lengths)*0.95)]}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "311-py-env",
      "language": "python",
      "name": "311-py-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
