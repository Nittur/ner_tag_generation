{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA SFT for Instruction-style NER on Qwen2.5-0.5B\n",
        "\n",
        "This notebook fine-tunes the locally downloaded `Qwen2.5-0.5B` with PEFT LoRA in 4-bit (QLoRA) for instruction-style NER via causal language modeling. The dataset is the standardized JSONL with `instruction` and `response` fields; training text is `<instruction>\\n<response>`. The LoRA adapter is saved separately for later merging or on-device inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Pavan Nittur\\Coding\\ML\\311-py-env\\Lib\\site-packages\\~%kenizers'.\n",
            "  You can safely remove it manually.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Torch: 2.5.1+cu121 CUDA: 12.1 is_available: True\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade -q pip wheel setuptools\n",
        "%pip install -q torch --index-url https://download.pytorch.org/whl/cu124\n",
        "%pip install -q transformers==4.45.2 datasets==3.0.1 peft==0.13.2 accelerate==1.0.1 trl==0.11.4 evaluate==0.4.3 scipy==1.13.1\n",
        "\n",
        "import torch\n",
        "print('Torch:', torch.__version__, 'CUDA:', torch.version.cuda, 'is_available:', torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model and tokenizer from models/Qwen2.5-0.5B\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_dir = \"models/Qwen2.5-0.5B\"  # local path\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(\"Loaded model and tokenizer from\", model_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 8,798,208 || all params: 502,588,160 || trainable%: 1.7506\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_targets = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=lora_targets,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "727f15ee64d142f1aacd41fc7007bee9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bf35694a525462e8ce0920e1a403f88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fab93a266f5b492cb97ce7bb88967349",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d6a3a4d3eae47dbbafa36697adba6e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "245f0c8370774d6f871496fe19a2e566",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42e8a122eab046999d8051cbb35043d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 20\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 10\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 10\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "train_path = \"outputs/standardized_gretel_pii_masking_en_train.jsonl\"\n",
        "val_path = \"outputs/standardized_gretel_pii_masking_en_validation.jsonl\"\n",
        "test_path = \"outputs/standardized_gretel_pii_masking_en_test.jsonl\"\n",
        "\n",
        "datasets = DatasetDict({\n",
        "    \"train\": load_dataset(\"json\", data_files=train_path, split=\"train\"),\n",
        "    \"validation\": load_dataset(\"json\", data_files=val_path, split=\"train\"),\n",
        "    \"test\": load_dataset(\"json\", data_files=test_path, split=\"train\"),\n",
        "})\n",
        "\n",
        "concat_key = \"text\"\n",
        "for split in [\"train\", \"validation\", \"test\"]:\n",
        "    datasets[split] = datasets[split].map(\n",
        "        lambda ex: {concat_key: ex[\"instruction\"] + \"\\n\" + ex[\"response\"]},\n",
        "        remove_columns=datasets[split].column_names,\n",
        "    )\n",
        "\n",
        "print(datasets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train file: outputs/standardized_gretel_pii_masking_en_train.jsonl\n",
            "Loaded 2 sample(s) from JSONL for preview\n",
            "\n",
            "--- Sample 0 ---\n",
            "instruction (first 200 chars):\n",
            " You will label tokens with BIO tags. Output only the tag after each token. One token per line.\\n\\nText:\\n**ADOPTION CERTIFICATE**\\nIssued by Guernsey Adoption Agency, this certificate confirms the adoptio\n",
            "\n",
            "response (first 200 chars):\n",
            " O\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nB-COUNTRY\\nI-COUNTRY\\nI-COUNTRY\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nB-NAME\\nI-NAME\\nI-NAME\\nI-NAME\\nI-NAME\\nO\\nO\\nO\\nO\\nB-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_B\n",
            "\n",
            "concat length: 1134\n",
            "\n",
            "--- Sample 1 ---\n",
            "instruction (first 200 chars):\n",
            " You will label tokens with BIO tags. Output only the tag after each token. One token per line.\\n\\nText:\\n**Account Closure Form**\\n\\n**Account Holder:**\\n- **Name:** Zashil Tripathi\\n- **Date of Birth:** 200\n",
            "\n",
            "response (first 200 chars):\n",
            " O\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nB-NAME\\nI-NAME\\nI-NAME\\nI-NAME\\nI-NAME\\nI-NAME\\nO\\nO\\nO\\nO\\nO\\nO\\nO\\nB-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DATE_OF_BIRTH\\nI-DA\n",
            "\n",
            "concat length: 1303\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DEBUG PREVIEW — raw instruction/response and concatenated text\n",
        "import json\n",
        "from itertools import islice\n",
        "\n",
        "print(\"Train file:\", train_path)\n",
        "\n",
        "def peek_jsonl(path, n=2):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in islice(f, n):\n",
        "            try:\n",
        "                rows.append(json.loads(line))\n",
        "            except Exception as e:\n",
        "                print(\"Parse error:\", e)\n",
        "    return rows\n",
        "\n",
        "samples = peek_jsonl(train_path, n=2)\n",
        "print(f\"Loaded {len(samples)} sample(s) from JSONL for preview\\n\")\n",
        "\n",
        "for i, ex in enumerate(samples):\n",
        "    instr = ex.get(\"instruction\", \"\")\n",
        "    resp = ex.get(\"response\", \"\")\n",
        "    concat_text = instr + \"\\n\" + resp\n",
        "    print(f\"--- Sample {i} ---\")\n",
        "    print(\"instruction (first 200 chars):\\n\", instr[:200].replace(\"\\n\", \"\\\\n\"))\n",
        "    print(\"\\nresponse (first 200 chars):\\n\", resp[:200].replace(\"\\n\", \"\\\\n\"))\n",
        "    print(\"\\nconcat length:\", len(concat_text))\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw concatenated examples (first 2):\n",
            "\n",
            "--- Example 0 length=1134 ---\n",
            "You will label tokens with BIO tags. Output only the tag after each token. One token per line.\\n\\nText:\\n**ADOPTION CERTIFICATE**\\nIssued by Guernsey Adoption Agency, this certificate confirms the adoption of Urvashi Jaggi, born on 2015-07-26, by the adoptive parents. The adoption was finalized on 2022-\n",
            "\n",
            "--- Example 1 length=1303 ---\n",
            "You will label tokens with BIO tags. Output only the tag after each token. One token per line.\\n\\nText:\\n**Account Closure Form**\\n\\n**Account Holder:**\\n- **Name:** Zashil Tripathi\\n- **Date of Birth:** 2008-11-18\\n- **SSN:** 819-46-1268\\n\\n**Account Details:**\\n- **Account Number:** G42778036994\\n\\n**Reason fo\n",
            "\n",
            "Tokenized preview keys: ['input_ids', 'attention_mask']\n",
            "input_ids lens: [515, 652]\n",
            "\n",
            "First 30 token ids (ex0): [2610, 686, 2383, 11211, 448, 72066, 9492, 13, 9258, 1172, 279, 4772, 1283, 1817, 3950, 13, 3776, 3950, 817, 1555, 382, 1178, 510, 334, 28814, 6578, 62357, 82023, 1019, 28216]\n",
            "First 30 tokens (ex0): ['You', 'Ġwill', 'Ġlabel', 'Ġtokens', 'Ġwith', 'ĠBIO', 'Ġtags', '.', 'ĠOutput', 'Ġonly', 'Ġthe', 'Ġtag', 'Ġafter', 'Ġeach', 'Ġtoken', '.', 'ĠOne', 'Ġtoken', 'Ġper', 'Ġline', '.ĊĊ', 'Text', ':Ċ', '**', 'ADO', 'PTION', 'ĠCERT', 'IFICATE', '**Ċ', 'Iss']\n",
            "\n",
            "Collated batch tensors:\n",
            "input_ids torch.Size([2, 656]) torch.int64\n",
            "attention_mask torch.Size([2, 656]) torch.int64\n",
            "labels torch.Size([2, 656]) torch.int64\n",
            "\n",
            "labels slice ex0: [2610, 686, 2383, 11211, 448, 72066, 9492, 13, 9258, 1172, 279, 4772, 1283, 1817, 3950, 13, 3776, 3950, 817, 1555, 382, 1178, 510, 334, 28814, 6578, 62357, 82023, 1019, 28216]\n"
          ]
        }
      ],
      "source": [
        "# DEBUG PREVIEW — tokenization and collator mini-batch\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Use local defaults if globals not defined yet\n",
        "dbg_max_length = max_length if 'max_length' in globals() else 1024\n",
        "\n",
        "# Take first 2 concatenated examples from datasets[\"train\"] before tokenization\n",
        "raw_preview = datasets[\"train\"].select(range(min(2, len(datasets[\"train\"]))))\n",
        "print(\"Raw concatenated examples (first 2):\\n\")\n",
        "for i in range(len(raw_preview)):\n",
        "    txt = raw_preview[i][\"text\"]\n",
        "    print(f\"--- Example {i} length={len(txt)} ---\")\n",
        "    print(txt[:300].replace(\"\\n\", \"\\\\n\"))\n",
        "    print()\n",
        "\n",
        "# Tokenize those two directly to inspect ids/tokens\n",
        "_tok = tokenizer(list(raw_preview[\"text\"]), truncation=True, max_length=dbg_max_length, padding=False, return_attention_mask=True)\n",
        "print(\"Tokenized preview keys:\", list(_tok.keys()))\n",
        "print(\"input_ids lens:\", [len(x) for x in _tok[\"input_ids\"]])\n",
        "\n",
        "# Show first 30 tokens and ids for example 0\n",
        "ids0 = _tok[\"input_ids\"][0]\n",
        "print(\"\\nFirst 30 token ids (ex0):\", ids0[:30])\n",
        "print(\"First 30 tokens (ex0):\", tokenizer.convert_ids_to_tokens(ids0[:30]))\n",
        "\n",
        "# Build a mini-batch using a local collator (pads and creates labels)\n",
        "collator_local = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8)\n",
        "\n",
        "batch_features = []\n",
        "for i in range(len(_tok[\"input_ids\"])):\n",
        "    batch_features.append({\n",
        "        \"input_ids\": _tok[\"input_ids\"][i],\n",
        "        \"attention_mask\": _tok[\"attention_mask\"][i],\n",
        "    })\n",
        "\n",
        "batch = collator_local(batch_features)\n",
        "print(\"\\nCollated batch tensors:\")\n",
        "for k, v in batch.items():\n",
        "    try:\n",
        "        print(k, v.shape, v.dtype)\n",
        "    except Exception:\n",
        "        print(k, type(v))\n",
        "\n",
        "# Show a short slice of labels to confirm LM objective alignment\n",
        "print(\"\\nlabels slice ex0:\", batch[\"labels\"][0][:30].tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58089c7c69ab47e4acd76842eaac99c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train:   0%|          | 0/20 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "359818674e734a59a1ef7eb9be8349b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing validation:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c84a0ef4b456448c91715abe1074d7dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing test:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "max_length = 1024\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    out = tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=False,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    return out\n",
        "\n",
        "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "    tokenized = {}\n",
        "    for split in [\"train\", \"validation\", \"test\"]:\n",
        "        tokenized[split] = datasets[split].map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=datasets[split].column_names,\n",
        "            desc=f\"Tokenizing {split}\",\n",
        "        )\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train examples: 20\n",
            "Eval examples: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pavan Nittur\\Coding\\ML\\311-py-env\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import math, os\n",
        "\n",
        "run_name = \"qwen25-0.5b-qlora-ner\"\n",
        "output_dir = os.path.join(\"outputs\", run_name)\n",
        "\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 4\n",
        "num_train_epochs = 3\n",
        "learning_rate = 2e-4\n",
        "warmup_ratio = 0.03\n",
        "weight_decay = 0.0\n",
        "logging_steps = 10\n",
        "eval_steps = 200\n",
        "save_steps = 200\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    run_name=run_name,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_steps=logging_steps,\n",
        "    eval_steps=eval_steps,\n",
        "    save_steps=save_steps,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=learning_rate,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    weight_decay=weight_decay,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_torch\",\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    torch_compile=False,\n",
        "    report_to=[\"none\"],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "print(\"Train examples:\", len(tokenized[\"train\"]))\n",
        "print(\"Eval examples:\", len(tokenized[\"validation\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "124f66ab967444778cf44d406f3e58f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pavan Nittur\\Coding\\ML\\311-py-env\\Lib\\site-packages\\peft\\utils\\save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 9.81, 'train_samples_per_second': 6.116, 'train_steps_per_second': 0.306, 'train_loss': 1.1724990208943684, 'epoch': 2.4}\n",
            "***** train metrics *****\n",
            "  epoch                    =        2.4\n",
            "  total_flos               =    73963GF\n",
            "  train_loss               =     1.1725\n",
            "  train_runtime            = 0:00:09.81\n",
            "  train_samples_per_second =      6.116\n",
            "  train_steps_per_second   =      0.306\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "train_result = trainer.train()\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pavan Nittur\\Coding\\ML\\311-py-env\\Lib\\site-packages\\peft\\utils\\save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved LoRA adapter to: outputs\\qwen25-0.5b-qlora-ner\\lora_adapter\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "adapter_dir = os.path.join(output_dir, \"lora_adapter\")\n",
        "os.makedirs(adapter_dir, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(adapter_dir)\n",
        "\n",
        "tokenizer.save_pretrained(os.path.join(adapter_dir, \"tokenizer\"))\n",
        "\n",
        "print(\"Saved LoRA adapter to:\", adapter_dir)\n",
        "\n",
        "# Example: how to load later for inference without altering base model\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, quantization_config=bnb_config)\n",
        "# peft_model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
        "# peft_model.eval()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "311-py-env",
      "language": "python",
      "name": "311-py-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
